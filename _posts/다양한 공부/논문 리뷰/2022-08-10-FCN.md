---
title: "논문 리뷰: Fully Convolutional Networks for Semantic Segmentation(2015)"
excerpt : FCN에 대한 논문 리뷰
categories:
  - 논문 리뷰
  - Computer Vision
  - Pattern Recognition
  - CNN
toc: true

---

[본 논문 FCN](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)을 읽고 내용을 정리한 논문입니다.

---

# 1. _Introduction_

CNN이 사용됨으로써 bounding box detection, keypoint prediction과 같은 많은 장르들에 발전이 이뤄졌다.

이제 자연스럽게 다음 스텝으론 픽셀마다 label을 예측하는 단계에 접어든다.

저자들의 네트워크 FCN은 end-to-end 학습을 진행하고, pixels-to-pixels 학습을 진행한다. 당시엔 FCN이 저자들이 알기론 픽셀 단위로 예측을 하는 첫 end-to-end 네트워크였다고 한다.

Patch단위의 학습은 fully convolutional traning의 효율성이 결여되어있다. 그리고 저자들의 방법은 전,후처리 따위 없이 진행될 것이고 fully convolutional and fine-tuning 을 통해 
좋은 분류기로써의 성능을 보여줄 것이다.

Semantic segmentation에서, global information은 이 물체가 **무엇**인지 판단하는데 도움을 주고, local information은 이 물체가 **어디**에 있는지 판단하는데 도움을 준다.
깊은 특징 구조는 location과 semantic을 같이 local-to-global pyramid구조에서 해석되는데, 저자들은 이런 정보들을 합치고 싶어했던 것 같다.
그래서 일명 **skip** 구조를 정의해서 deep,coarse,semantic 정보들과 shallow,fine,appearance 정보들을 합치고자 한다.

<br/>

# 3. Fully convolutional networks 

기본적으로 데이터의 각 layer들은 $h\times w \times d$구조로 되어있다. 이미지를 예시로 들면 $height \times width \times channel(RGB\;or\;grayscale)$로 표현된다.

| ![ImageNet 데이터](/assets/images/다양한 공부/논문/OverFeat/imagenet데이터.png) | 
|:--:| 
| *ImageNet 데이터들 (출처 : https://www.tensorflow.org/datasets/catalog/imagenet2012)* |


실험은 ImageNet의 데이터를 썻는데, 이 데이터셋의 이미지들은 대부분 이미지에 꽉차게 물체가 가운데에 위치하는 것을 볼 수 있다.
그러나 진짜 이미지에서 물체의 크기나 위치는 때때로 달라진다. 문제를 해결하기위한 첫번째 아이디어는 다음과 같다.

* Convnet을 여러 크기들의 window로 이미지의 여러 곳에 적용시키는 것

위 방법은 classification에는 효율적이나, 물체의 전체 혹은 물체의 중심을 학습하는 것은 아니기에, localization과 detection에 있어서 형편없다.
그래서 이를 해결하기 위한 두번째 아이디어는 다음과 같이 학습하는 것이다.

* 각각의 window마다 카테고리들의 distribution을 만들기
* location 예측과 물체를 포함하는 bounding box의 크기를 예측하는 것

세번째 아이디어는 다음과 같다.

* 각각의 카테고리들에 대한 evidence를 각각의 location과 size에서 축적하는 것이다. 

<br/><br/><br/>

# 2. _Vision Tasks_
Compute vision task에 있어서 저자는 classification -> localization -> detection 순으로
난이도가 증가한다고 하며, -> 방향으로 이전의 task는 다음 task의 종속된 task느낌이라고 한다. 

2013년에 열린 ImageNet 대회를 예를 들어보자. 이 대회에선 각각의 task마다 데이터셋도 조금 다르고
목표도 좀 다른 것 같음(?)

<br/>

## 2.1 Classification

각각의 이미지들은 이미지의 main object에 맞는 단일 라벨이 할당되어있다. 올바른 선택을 위해 5개의 예측까지를 허용된다.

<br/>

## 2.2 Localization

이것은 한 이미지당 5개의 추측을 낼 수 있지만, 추가적으로 각각의 추측마다 bounding box도 내야한다.
올바른 답을 고려하는 기준은 bounding box의 50%이상이 맞고 classification 또한 맞아야 한다.

<br/>

## 2.3 Detection

이것은 localization과 살짝 다르다. 각각의 이미지마다 object가 있을수도, 아님 많을 수도 있다. 
그리고 false positive들(예를 들어, 이미지에 사람이 없는데 사람이 있다고 예측하는 것)은 mean average precision(mAP)에 의해 패널티를 받는다.

<br/><br/><br/>

# 3. _Classification_
저자들의 모델은 ILSVRC12대회의 best model보다 network design이나 inference step을 향상시켰다.
이것에 대한 설명은 나중에 보자.

<br/>

## 3.1 Model Design and Training

저자들은 학습을 위해 1000개의 클래스를 갖는 1,200,000개의 ImageNet 2012 데이터 셋을 썼다.
초기 설정 및 입력들은 다음과 같이 했다.

* 처음 input의 size는 고정되어있지만, calssfication을 위해 multi-scale을 취했다.
* 각각의 이미지들은 최소 256 pixel들을 갖게 했으며 추가적으로 221x221 사이즈로 이미지를 5번 랜덤하게 잘라냈다.
* Size of [mini-batch](https://yhyuntak.github.io/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%B0%EC%B9%98-%ED%95%99%EC%8A%B5/) : 128
* Initial weight : randomly with $(\mu,\sigma)=(0,1\times10^{-2})$
* Optimizer : Stochasitc Gradient Descent([SGD](https://yhyuntak.github.io/%EB%94%A5%EB%9F%AC%EB%8B%9D/optimizer/SGD/))
* momentum : 0.6, $l_2$ weight decay : 1e-5
* learning rate : 초기값은 5e-2이고, 30,50,60,70,80 epoch가 진행되면서 0.5를 곱했다.
* DropOut(rate=0.5)를 fully connected layer 6,7층에 배치

네트워크의 구조에 대한 자세한 설명은 다음 사진에 정리되어있다.
| ![네트워크 구조](/assets/images/다양한 공부/논문/OverFeat/table1.png) |
|:--: |
| * _fast_ 네트워크의 구조(후술함)* |

학습하는 동안에는 저자들은 네트워크를 non-spatial(output map을 1x1로)하게 다뤘다.
그리고 layer 1~5는 [Krizhevsky][Krizhevsky]과 유사하게 relu, max pooling등을 사용했지만, 다른 점은 다음과 같다.

* contrast normaliztion을 사용하지 않음. 
* pooling region을 overlapping하지 않음.
* **stride를 1/2으로 줄임으로써 layer 1,2의 크기를 키움.**

이렇게 한 이유는, stride가 클수록 속도는 빠르지만 정확도에 안좋은 영향을 끼치기 때문이다. 아래 사진처럼
각 layer들은 이미지에 대한 다양한 feature들(orientated edge, pattern, blob 등)을 갖기 때문에, stride를 통해
이런 feature들을 지워버리면 당연히 학습에 안좋은 영향을 미칠 수 밖에 없다.

| ![feature들](/assets/images/다양한 공부/논문/OverFeat/figure2.png) |
| :--: |
| *layer 1,2의 feature 예시* |

<br/>

## 3.2 Feature Extractor

우리는 강력한 feature들을 제공하는 **OverFeat**이라고 불리는 feature extractor를 공개한다. 
2개의 모델이 있는데, 하나는 _fast_ 이고, 다른 하나는 _accurate_ 이다. _fast_ 는 3.1에 이미 업로드되어있다. 
나머지 _accurate_ 는 다음과 같다.

| ![feature들](/assets/images/다양한 공부/논문/OverFeat/accurate.png) |
| :--: |
| * _accurate_ 모델의 예시 * |

이름에서 알다시피 *accurate* 이 *fast* 보다 정확도는 높지만, 그만큼 connection이 2배 많다.

## 3.3 Multi-Scale Classification

[Krizhevsky][Krizhevsky] 의 논문에선 10개의 고정된 view들(4개의 코너, 센터 그리고 이것들의 수평반전들)의 평균을 이용해서 
성능을 증진시켰다. 그러나 이건 이미지가 overlap되면서 계산적으로 이익이 없다. 

그래서 저자들은 이미지의 각 location에 대해 다양한 scale을 적용하기로 했다. 
sliding window의 방식은 ConvNet에서 굉장히 효율적이다. 이것은 voting을 위한 많은 view들을 제공해준다. voting을 통해 우리는
효율성을 유지하면서 robustness 또한 증가시킨다. 임의의 크기의 이미지에 ConvNet을 적용하면, 각각의 scale에 따른
C-dimentional vector들의 spatial map이 생성된다.

위 네트워크에서 전체 subsampling의 비율이 2x3x2x3 또는 36이라는데.. 이부분은 잘 이해가 가지 않는다.(정확히 위 네트워크에서 어떻게 2x3x2x3을 뽑는질 모르겠다)
어쨌든.. 이런 것때매 성능이 하향되는데, 그 이유는 네트워크의 window들이 object에 맞춰서 잘 조정되어 있지 않기 때문이다.
이게 잘 되어있을 수록, 네트워크의 신뢰도(confidence)가 강해진다고 한다.
이런 문제를 해결하기 위해 [Giusti][Giusti]의 논문과 유사하게 last subsampling operation을 매 offset마다 적용한다.

이제 

# Reference

[Krizhevsky] : https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

[Giusti] : https://arxiv.org/abs/1302.1700

[Google] : google.com