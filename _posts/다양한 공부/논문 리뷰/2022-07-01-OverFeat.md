---
title: "논문 리뷰: OverFeat(2014)"
excerpt : OverFeat에 대한 논문 리뷰
categories:
  - 논문 리뷰
  - Computer Vision
  - Pattern Recognition
  - CNN
toc: true

---

[본 논문 OverFeat](https://arxiv.org/pdf/1312.6229.pdf)은 [FCN 논문](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)을 읽던 도중 읽어봐야겠다고 생각되어 읽게 된 논문입니다.

---

# 1. _Introduction_

작은 데이터셋을 가진 CNN네트워크는 점점 분류기의 정확도가 좋아졌지만, 큰 데이터셋을 가진 네트워크에 비빌 수 없었다.

ConvNet들의 가장 큰 이점은 **전체 시스템을 _end to end_ 로 학습**하는 것이고, 
주된 단점은 **라벨링 데이터가 너무 많이 필요**하다는 것이다.

이 논문의 가장 주목해야할 점은 **classification과 localization,detection을 하나의 ConvNet으로 학습하는 것**이다.
특히, localization과 detection은 예측된 bounding box들을 쌓아서 할 것이다. localization 예측들을 막 합쳐보면, 
detection을 학습하지 않고 수행할 수 있기에 시간적으로 이득을 보고, 복잡한 bootstrapping 학습 과정을 거치지 않아도 된다.
(아마도 이 논문을 기준으로, 이전의 방법들은 3개의 목적을 달성하기 위해 꽤나 복잡한 과정을 거친 듯 하다.)

<br/>

## 1.1 데이터셋의 단점들과 해결을 위한 idea들

| ![ImageNet 데이터](/assets/images/다양한 공부/논문/OverFeat/imagenet데이터.png) | 
|:--:| 
| *ImageNet 데이터들 (출처 : https://www.tensorflow.org/datasets/catalog/imagenet2012)* |


실험은 ImageNet의 데이터를 썻는데, 이 데이터셋의 이미지들은 대부분 이미지에 꽉차게 물체가 가운데에 위치하는 것을 볼 수 있다.
그러나 진짜 이미지에서 물체의 크기나 위치는 때때로 달라진다. 문제를 해결하기위한 첫번째 아이디어는 다음과 같다.

* Convnet을 여러 크기들의 window로 이미지의 여러 곳에 적용시키는 것

위 방법은 classification에는 효율적이나, 물체의 전체 혹은 물체의 중심을 학습하는 것은 아니기에, localization과 detection에 있어서 형편없다.
그래서 이를 해결하기 위한 두번째 아이디어는 다음과 같이 학습하는 것이다.

* 각각의 window마다 카테고리들의 distribution을 만들기
* location 예측과 물체를 포함하는 bounding box의 크기를 예측하는 것

세번째 아이디어는 다음과 같다.

* 각각의 카테고리들에 대한 evidence를 각각의 location과 size에서 축적하는 것이다. 

<br/><br/><br/>

# 2. _Vision Tasks_
Compute vision task에 있어서 저자는 classification -> localization -> detection 순으로
난이도가 증가한다고 하며, -> 방향으로 이전의 task는 다음 task의 종속된 task느낌이라고 한다. 

2013년에 열린 ImageNet 대회를 예를 들어보자. 이 대회에선 각각의 task마다 데이터셋도 조금 다르고
목표도 좀 다른 것 같음(?)

<br/>

## 2.1 Classification

각각의 이미지들은 이미지의 main object에 맞는 단일 라벨이 할당되어있다. 올바른 선택을 위해 5개의 예측까지를 허용된다.

<br/>

## 2.2 Localization

이것은 한 이미지당 5개의 추측을 낼 수 있지만, 추가적으로 각각의 추측마다 bounding box도 내야한다.
올바른 답을 고려하는 기준은 bounding box의 50%이상이 맞고 classification 또한 맞아야 한다.

<br/>

## 2.3 Detection

이것은 localization과 살짝 다르다. 각각의 이미지마다 object가 있을수도, 아님 많을 수도 있다. 
그리고 false positive들(예를 들어, 이미지에 사람이 없는데 사람이 있다고 예측하는 것)은 mean average precision(mAP)에 의해 패널티를 받는다.

<br/><br/><br/>

# 3. _Classification_
저자들의 모델은 ILSVRC12대회의 best model보다 network design이나 inference step을 향상시켰다.
이것에 대한 설명은 나중에 보자.

<br/>

## 3.1 Model Design and Training

저자들은 학습을 위해 1000개의 클래스를 갖는 1,200,000개의 ImageNet 2012 데이터 셋을 썼다.
초기 설정 및 입력들은 다음과 같이 했다.

* 처음 input의 size는 고정되어있지만, calssfication을 위해 multi-scale을 취했다.
* 각각의 이미지들은 최소 256 pixel들을 갖게 했으며 추가적으로 221x221 사이즈로 이미지를 5번 랜덤하게 잘라냈다.
* Size of [mini-batch](https://yhyuntak.github.io/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%B0%B0%EC%B9%98-%ED%95%99%EC%8A%B5/) : 128
* Initial weight : randomly with $(\mu,\sigma)=(0,1\times10^{-2})$
* Optimizer : Stochasitc Gradient Descent([SGD](https://yhyuntak.github.io/%EB%94%A5%EB%9F%AC%EB%8B%9D/optimizer/SGD/))
* momentum : 0.6, $l_2$ weight decay : 1e-5
* learning rate : 초기값은 5e-2이고, 30,50,60,70,80 epoch가 진행되면서 0.5를 곱했다.
* DropOut(rate=0.5)를 fully connected layer 6,7층에 배치

네트워크의 구조에 대한 자세한 설명은 다음 사진에 정리되어있다.
| ![네트워크 구조](/assets/images/다양한 공부/논문/OverFeat/table1.png) |
|:--: |
| * _fast_ 네트워크의 구조(후술함)* |

학습하는 동안에는 저자들은 네트워크를 non-spatial(output map을 1x1로)하게 다뤘다.
그리고 layer 1~5는 [Krizhevsky][Krizhevsky]과 유사하게 relu, max pooling등을 사용했지만, 다른 점은 다음과 같다.

* contrast normaliztion을 사용하지 않음. 
* pooling region을 overlapping하지 않음.
* **stride를 1/2으로 줄임으로써 layer 1,2의 크기를 키움.**

이렇게 한 이유는, stride가 클수록 속도는 빠르지만 정확도에 안좋은 영향을 끼치기 때문이다. 아래 사진처럼
각 layer들은 이미지에 대한 다양한 feature들(orientated edge, pattern, blob 등)을 갖기 때문에, stride를 통해
이런 feature들을 지워버리면 당연히 학습에 안좋은 영향을 미칠 수 밖에 없다.

| ![feature들](/assets/images/다양한 공부/논문/OverFeat/figure2.png) |
| :--: |
| *layer 1,2의 feature 예시* |

<br/>

## 3.2 Feature Extractor

우리는 강력한 feature들을 제공하는 **OverFeat**이라고 불리는 feature extractor를 공개한다. 
2개의 모델이 있는데, 하나는 _fast_ 이고, 다른 하나는 _accurate_ 이다. _fast_ 는 3.1에 이미 업로드되어있다. 
나머지 _accurate_ 는 다음과 같다.

| ![feature들](/assets/images/다양한 공부/논문/OverFeat/accurate.png) |
| :--: |
| * _accurate_ 모델의 예시 * |

이름에서 알다시피 *accurate* 이 *fast* 보다 정확도는 높지만, 그만큼 connection이 2배 많다.

## 3.3 Multi-Scale Classification

[Krizhevsky][Krizhevsky] 의 논문에선 10개의 고정된 view들(4개의 코너, 센터 그리고 이것들의 수평반전들)의 평균을 이용해서 
성능을 증진시켰다. 그러나 이건 이미지가 overlap되면서 계산적으로 이익이 없다. 

그래서 저자들은 이미지의 각 location에 대해 다양한 scale을 적용하기로 했다. 
sliding window의 방식은 ConvNet에서 굉장히 효율적이다. 이것은 voting을 위한 많은 view들을 제공해준다. voting을 통해 우리는
효율성을 유지하면서 robustness 또한 증가시킨다. 임의의 크기의 이미지에 ConvNet을 적용하면, 각각의 scale에 따른
C-dimentional vector들의 spatial map이 생성된다.

위 네트워크에서 전체 subsampling의 비율이 2x3x2x3 또는 36이라는데.. 이부분은 잘 이해가 가지 않는다.(정확히 위 네트워크에서 어떻게 2x3x2x3을 뽑는질 모르겠다)
어쨌든.. 이런 것때매 성능이 하향되는데, 그 이유는 네트워크의 window들이 object에 맞춰서 잘 조정되어 있지 않기 때문이다.
이게 잘 되어있을 수록, 네트워크의 신뢰도(confidence)가 강해진다고 한다.
이런 문제를 해결하기 위해 [Giusti][Giusti]의 논문과 유사하게 last subsampling operation을 매 offset마다 적용한다.

[Google]


# Reference

[Krizhevsky] : https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

[Giusti] : https://arxiv.org/abs/1302.1700

[Google] : google.com