---
title: "논문 공부: Fast R-CNN(2015)"
excerpt : fast R-CNN에 대해 알아보자.
categories:
  - 컴퓨터 비전
  - 논문 리뷰
  - object detection
toc: true

---

본 글은 [논문 fast R-CNN](https://arxiv.org/abs/1504.08083)을 읽고 공부하는 글입니다.
이전 글 [R-CNN](https://yhyuntak.github.io/%EC%BB%B4%ED%93%A8%ED%84%B0%20%EB%B9%84%EC%A0%84/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/object%20detection/R-CNN/)에 이어 2번째 시리즈네요.

---

R-CNN에서의 문제점(?)
* 많은 region proposal들이 생성된다.
* region들이 물체의 대략적인 위치를 제공하기 때문에, 정확한 localization이 이뤄질 수 있어야함.

저자들은 단일 stage 학습 알고리즘으로 object proposal을 분류하는 것과 위치를 더 정확히 찾는 것을 제안함.

R-CNN의 문제점
1. 학습이 여러 단계를 거쳐야함.
  R-CNN은 먼저 CNN을 fine-tune해야하고 분류를 위해 SVM을 거친다. 그리고 마지막으로 bbox regressor을 학습한다.
2. 학습할 때, 자원과 시간을 많이 사용함.
  학습할 때, 2000장의 region proposal을 메모리에 저장하고 네트워크를 만드는데, 만약 깊은 네트워크를 사용하게 되면, storage에 문제가 생긴다.
3. object detection 속도가 느리다.
  feature들이 이미지 하나에서 2000개의 proposal로부터 추출되니 느릴 수 밖에 없다.

저자들은 이런 문제들을 보완하고자 Fast R-CNN을 제시함. 장점은 다음과 같음
1. R-CNN보다 더 높은 detection 정확도
2. 학습이 1개의 단계로 이루어져있음. 
3. 학습은 모든 네트워크 layer들을 업데이트할 수 있음.
4. feature를 얻기 위해 disk storage에 저장안해도 됌.

Fast R-CNN의 기본 구조는 그림 1과 같다.

|![그림 1](\assets\images\다양한 공부\논문\컴퓨터 비전\Fast R-CNN\그림 1.png)|
|:--:|
|_그림 1_|

구조에 대해 알아보자. 

Fast R-CNN은 입력으로 전체 이미지와 object proposal set을 쓴다.

과정은 다음과 같다.
1. 먼저 전체 이미지에 대해서 Conv layer들과 max pooling을 써서 feature map을 추출한다.
2. 각각의 object proposal에 대해 region of interst(RoI) pooling layer는 고정된 feature vector를 추출한다.
3. 각각의 feature vector들은 fc layer로 들어가서 class 분류를 위한 softmax와 bbox regressor에 사용된다. 이 때, 클래스에는 background도 포함된다. \
그리고 bbox regressor에서는 4개의 값이 나온다.

RoI pooling layer는 어떤 RoI의 feature든지 HxW의 고정된 작은 feature map으로 변환하기 위해 max pooling을 사용한다.
H,W는 hyper parameter다. 앞으로 RoI는 Conv feature map에 대한 직사각형의 window이다. RoI는 4개의 튜플 값$(r,c,h,w)$으로 이루어지고, r,c는 좌측 상단 코너의 좌표, h,w는 높이와 너비를 얘기한다.

RoI max pooling은  h x w RoI window를 H x W 그리드의 서브 윈도우로 나눈다. 그럼 크기는 대략 h/H x w/W가 된다... 알고보니 그냥 max pooling의 역할을 얘기한다.

pre-trained ImageNet network들로 실험을 하는데, 세션 4.1에 자세히 설명이 되어있다. 사전 학습된 네트워크들로 Fast R-CNN을 사용할 때, 3가지 변형을 줘서 그림 1의 구조를 만든다.
1. 마지막 max pooling layer는 RoI pooling layer로 바뀐다.
2. 네트워크의 마지막 fc layer와 softmax는 클래스 구분을 위한 softmax와 bbox regressor의 sibling layer들로 대체된다. 
3. 이미지의 리스트들과 RoI의 리스트들을 사용하는 네트워크로 수정된다.

그리고 fine-tuning으로 모든 네트워크의 가중치들을 학습한다. 


Multi-task loss :
본 모델은 2개의 output layer를 갖고 있다. 첫번째 layer는 RoI당 $p=(p_0,...,p_K)$의 discrete probability distribution 를 출력한다.
softmax의 K+1 카테고리들의 출력 값이라 생각하면된다. 두번째 layer는 bbox regression offset $t^k = (t^k_x,t_y^k,t_w^k,t_h^k)$를 출력한다. 이것은 각각의 K object class들의 offset인데..
K라는걸 보면 모든 물체의 offset을 출력하는가? 싶다.. $t^k$는 scale-invariant translation과 object proposal에 대한 log-space height/width shift를 명시한다.

각각의 학습 RoI들은 ground-truth class $u$와 bbox target $v$를 갖고 있다. 2개의 output들이 있으니 multi-task loss는 다음과 같이 정의한다.

$$
L(p,u,t^u,v)=L_{\text{cls}}(p,u) + \lambda[u>=1]L_{\text{loc}}(t^u,v)
$$

※ $L_{\text{cls}}(p,u)=-\log p_u$ 로 정답 클래스 u에 해당하는 확률 $p_u$에 음의 로그를 취한 것이다.
※ Iverson bracket indicator 함수 $[u>=1]$은 u가 1 이상일 때 1로, 아니면 0을 표현한다. 이것은 background(u=0)과 object들을 나눠주기 위한 것이다. background는 정답 bbox offset이 없기 때문이다.


$L_{\text{loc}}(t^u,v)$는 아마.. regressor이니까.. 클래스 u에 대한 정답 bbox offset $v=(v_x,v_y,v_w,v_h)$와 예측 offset $t^u = (t^u_x,t_y^u,t_w^u,t_h^u)$의 mse가 아닐까?
알고보니 다음의 식으로 구성되더라.

$$
L_{\text{loc}}(t^u,v) = \sum_{i\in\{x,y,w,h\}} \text{smooth}_{L_1}(t_i^u-v_i)
$$

$$
\text{smooth}_{L_1}(x) = \begin{cases}
0.5x^2 & if |x| <1 \\
|x|-0.5 & otherwise,
\end{cases}
$$

