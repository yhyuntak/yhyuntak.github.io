---
title: "컴퓨터 비전 논문 방향"
excerpt : 본 글은 유명 학회 논문들의 abstract를 읽고 어떤 분야들이 있는지 보기 위한 곳이다.
categories:
  - 논문 리뷰
  - 학회 탐방
toc: true
---

컴퓨터 비전과 관련된 연구들이 뭐가 있는지 abstract만 읽으면서 방향을 알아보자.

# 2021 

## ICCV

[2021년 ICCV](https://openaccess.thecvf.com/ICCV2021)의 논문들을 내가 보고싶은 분야별로 검색해서 **그림을 보고 재밌어 보이는 것들 위주**로 보자.
너무 논문들이 많아서 다 볼 순 없다.

### Detection

* [Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chan_Entropy_Maximization_and_Meta_Classification_for_Out-of-Distribution_Detection_in_Semantic_ICCV_2021_paper.pdf)
  
  이 논문은 아마도 anormaly detection에 관련된 문제 중 Semantic Segementation의 Out-of-Distribution에 대한 이야기를 하는 것 같다.
  > [NOTE] 인터넷을 찾아보니, anormaly detection에 대해 잘 정리된 [호야님의 블로그](https://hoya012.github.io/blog/anomaly-detection-overview-1/)를 찾을 수 있었다. 도움이 많이 되는 사이트가 될 것 같다.

  실제 자율주행과 같은 상황에서 만약 학습한 적 없던 물체를 만났을 땐, 잘못된 인식을 할 수 있다. 그 때를 위해 OoD detection 개념으로 해결하는 듯하다. 
  OoD detection을 접근하는 기본적인 방법이 pixel-wise softmax entropy에 threshold를 잡는 것인데, 이것을 좀 더 향상시키기 위해 2가지 단계로 표현한다고 한다.
  > [후기] anormaly detection 이란 개념에 대해 처음 알게 되었다. 이런 장르도 있구나 싶네.
  
  |![그림](\assets\images\다양한 공부\논문\컴퓨터 비전\방향정리\2021\ICCV\Detection\OoDdetection_entropy.png)|
  |:--:|
  |Figure 1|

  Figure 1에서 baseline의 segmentation mask에서 제일 아래부분이나 중심에 초록색 컨투어가 있는데, 이것은 OoD 데이터이기 때문에 딱 어느 하나의 segmentation이 되면 안될 것이다. 실제로 저자들의 방법의 결과를 보면,
  여러 segmentation으로 되어있고 heatmap이 빨갛게 된 것을 볼 수 있다. 아마 이런 느낌으로 해석하는게 맞을 듯..?

* [Specificity-preserving RGB-D Saliency Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Specificity-Preserving_RGB-D_Saliency_Detection_ICCV_2021_paper.pdf)

  이 논문은 RGB-D saliency detection에 대한 논문이다. 
  > [NOTE] saliency map이란, 관심이 있는 물체를 관심 없는 것들로부터 분리시키는 것을 이야기한다. 더 자세한 설명은 [유니디니님의 블로그](https://go-hard.tistory.com/3)를 참고하자. 
  
  기존의 관련 논문들은 다양한 fusion 전략을 통해서 shared representation을 학습하는데 집중하고, 극소수의 몇몇 방법들만 modality-specific 특성을 어떻게 보존할지 고민한다.
  이 논문은 저 특성을 보존하기 위해 새로운 관점을 제시하는 모양이다. 제시하는 SP-Net(specificity-preserving network)는 shared information과 modality-specific 특성을 고려해 성능 향상을 노린다. 
  > [NOTE] 여기서 modality-specific이 뭔지 모르겠다. modality는 흔히 multi-modality로 사용되는 단어인 것 같다. multi-modality란, 음성,텍스트,이미지 등 다양한 입력들을 결합하여 AI 알고리즘을 만들어 
  엄청난 성능 향상을 꾀하는 새로운 분야라고 한다. [출처](https://www.aimesoft.com/multimodalai.html) 이 관점에서 본다면, modality-specific은 아마 RGB와 Depth 이미지를 결합하는 것을 의미할 것 같다.
  
  |![그림](\assets\images\다양한 공부\논문\컴퓨터 비전\방향정리\2021\ICCV\Detection\rgbd_saliency_map.png)|
  |:--:|
  |Figure 1|

  Figure 1의 (c)를 보면, Conv layer로 Fusion 하는 module을 CIM(crossenhanced intergration module)이라 부르는 것 같고, Decoder에서 feature aggregation 하는 모듈을 multi-modal feature aggregation(MFA) module이라고 한다.
  저런 식으로 해서, 멀티모달 특징들을 상호보완해서 saliency map의 성능을 향상시키려는 것 같다.

* [Image Manipulation Detection by Multi-View Multi-Scale Supervision](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Image_Manipulation_Detection_by_Multi-View_Multi-Scale_Supervision_ICCV_2021_paper.pdf)

  |![그림](\assets\images\다양한 공부\논문\컴퓨터 비전\방향정리\2021\ICCV\Detection\manipul.png)|
  |:--:|
  |Figure 1|

  이 논문에 대해 이야기하기 전에, 먼저 Image Manipulation Detection에 대해 이야기해보자. 2018 CVPR 논문 [Learning Rich Features for Image Manipulation Detection](https://arxiv.org/pdf/1805.04953v1.pdf)에 따르면, 
  Image Manipulation Detection이란, 기존의 전통적인 semantic object detection과 다르다. 왜냐하면, image content보다 tampering artifact에 더 집중하기 때문이다. 그리고 이것을 학습하기 위해서 더 많은 feature들이 필요하다.
  이게 무슨 말..?!

  인터넷 서칭하고 곰곰히 생각해보고 이해한 바대로 다시 설명하자면, **이미지에서 조작된 부분을 찾는 것을 Image Manipulation Detection** 이라고 한다. tamper은 "함부로 변경하다/간섭하다"라는 뜻을 갖는다.
  그래서 tampering artifact란, 임의로 조작된 구조물이라고 생각하면 될 것 같다. 그럼 위에 내용이 이해가 간다. **조작된 물건에 더 집중해서 학습하려면, 많은 feature가 필요하다는 의미**이다.
  
  자 그럼, 다시 본 논문으로 돌아가 Abstract를 읽어보자.
  
  Image Manipulation Detection의 핵심 과제는 새로운 데이터의 조작된 것에 얼마나 민감하게 일반화된 feature를 학습하는 동시에 진짜 이미지에 대해선 잘못된 탐지를 막는 것이다.
  여태까지의 논문들은 저 민감함(sensitivity)에만 집중하고, specificity는 간과했다고 한다. 이 논문에서 저자들은 **mvfl(multi-view feature learning)과 mss(multi-scale supervision)**으로 이 둘을 다룰려고 하나보다. 
  
  noise distribution과 조작된 영역을 둘러싼 경계를 보면서, mvfl은 semantic-agnostic과 좀더 일반화된 feature들을 학습한다. 그리고 mss는 semantic segmentation 네트워크에 의해 고려되어야 할 중요한 실제 이미지로부터 학습한다.

* [Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Diomataris_Grounding_Consistency_Distilling_Spatial_Common_Sense_for_Precise_Visual_Relationship_ICCV_2021_paper.pdf)
  
  |![그림](\assets\images\다양한 공부\논문\컴퓨터 비전\방향정리\2021\ICCV\Detection\GRAPH.png)|
  |:--:|
  |Figure 1|
  
  와 이런 연구가 다 있다.. 이미지를 보고 사람(객체)과 물체(객체)에 대한 상호작용을 나타내는 그래프 관계도를 예측하는 것인가..!? 미쳤다 진짜~! 
  > [NOTE] [유니디니님의 블로그](https://go-hard.tistory.com/8)를 보면 2017 ICCV 논문 [Scene graph generation from objects, phrases and region captions](https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Scene_Graph_Generation_ICCV_2017_paper.pdf)가 
  이 연구의 아주 기본 연구인 것 같다. 

  매우 매우 흥미롭다..! 다시 본론으로 돌아와서, 그동안의 SGG들(Scene Graph Generator)은 subject와 object에 대한 데이터 세트의 bias(relationships' context)를 이용해 recall을 향상시키고,
  공간적, 시각적 정보를 소홀히 했다. 예를 들면, 사람들이 다들 셔츠를 입고있는 데이터를 학습했다고(bias)해서 모든 사람이 셔츠를 입을 것이라고 믿는 것처럼 말이다.
  이런 부정확한 예측들은 주로 대부분의 관계들에 대한 negative 예시들이 없어서 발생한다.  
  
  본 논문은 그동안의 SGG들이 이런 취약성을 보인다는 것을 깊게 조사했다.  그리고 이것을 예방하기 위해, closed-loop 방법으로 예측된 triplet들이 이미지에 일관되게 grounded되도록 semi-supervised 방법을 사용하낟.
  오.. 그 뒤로는 좀 내용들이 이해가 안가네.. 이런 연구가 있다는 것을 알았다는 것에 일단 만족하자.

### Segementation

[Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Garnot_Panoptic_Segmentation_of_Satellite_Image_Time_Series_With_Convolutional_Temporal_ICCV_2021_paper.pdf)

  
  |![그림](\assets\images\다양한 공부\논문\컴퓨터 비전\방향정리\2021\ICCV\Detection\land_panop.png)|
  |:--:|
  |Figure 1 - 가짜 물체 같은 것에 마스크가 되어있다.|

  multi-temporal satellite imagery가 무엇인지 정확히 모르겠다. 검색해보니 일정 기간동안 측정된 데이터를 이야기하는 것 같다. 그래서 이 논문은 
  픽셀 단위로 농경지(agricultural parcels)에 대한 panoptic segmentation을 하려고 하는 것 같다. 이것은 경제적/환경적으로 중요하다고 한다.
  
  많은 연구자들이 1장의 이미지에서만 결과를 가져오려 했지만, 저자들은 crop phenology의 복잡한 시간 패턴이 이미지의 시간적 시퀀스로 더 잘 해결된다고 한다. 
  저자들은 end-to-end로 single-stage method로 인공 위성 이미지 타임 시리즈의 panoptic segmentation을 진행한다. 그리고 이 모듈은 저자들의 새로운 이미지 시퀀스 encoding 네트워크과 결합될 것이라고 한다.
  이 네트워크는 풍부하고 적응적인 멀티스케일 spatiotemporal feature들을 추출하기 위해 temporal self-attention에 의존한다.
  