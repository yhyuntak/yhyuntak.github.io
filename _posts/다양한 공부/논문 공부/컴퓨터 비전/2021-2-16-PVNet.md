---
title: "논문 공부: PVNet : Pixel-wise Voting Network for 6DoF Pose Estimation"
excerpt : DenseFusion 공부
categories:
  - 컴퓨터 비전
  - 논문 리뷰
toc: true

---

본 글은 [논문 PVNet](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1812.11788.pdf)을 읽고 공부하는 글입니다.

---


# 1. _Introduction_
---

object pose estimation은 object들을 detect하고, canonical frame(?)에 대해 object들의 orientations, translation들을 예측하는 것이다.[42]
정확한 pose estimation들은 증강현실, 자율주행 자동차, 로봇 매니퓰레이션과 같은 application들에서 중요하다.
이 논문은 object의 6DoF pose를 추정하는 specific setting에 집중한다.
예를 들면, object가 있는 1개의 RGB 이미지로부터 3D rotation, orientation을 추정하는 것이다.

전통적인 방법들[27,23,18]은 object image와 object model간의 correspondence들을 가져가면서 pose estimation을 수행한다. 이들은 수작업으로 만든 특징들을 사용한다. 문제점은 이미지가 변하거나(variation) 배경이 복잡하면 (clutter) robust하지 않다.
end-to-end 학습을 하는 딥러닝 네트워크들[36,20,43,6]은 입력으로 이미지를 넣고, 이미지 속에서 물체에 맞는 pose를 출력한다. 그러나 generalization은 이슈로 남는다. 왜냐하면 이런 end-to-end 방법들은 pose estimation을 위한 적절한 feature 표현을 학습하는 것이 unclear하기 때문이다.
[32,33,39]은 (1) CNN을 통해 2D keypoints를 추출하고(regression) (2) PnP알고리즘을 사용하여 6D Pose parameter들을 계산한다.
이렇게 2단계로 나눈 방법은 keypoints를 찾는데에 있어서 robust하여 17년 당시 좋은 성능을 냈다. 그러나 겹쳐지거나 truncate(길이가 짧아진?) object들에 대해 잘 안되나 봄. 왜냐, keypoint들을 잘 못 찾기 때문이다.
CNN이 뭐 물체의 비슷한 패턴을 기억해서 보이지 않는 keypoint들을 예측한다 하더라도 이는 generalization되기 어렵다.

저자들은 그래서 occlusion과 truncation을 해결하기 위해선 "pixel-wise or patch-wise estimate"라고 불리는 dense predictin들이 "최종 출력" 혹은 중간의 표현을 위해 필요하다.
저자들의 네트워크는 2D keypoint를 regression으로 하는 것 대신에 각 픽셀에서 object의 keypoint들로 향하는 방향을 가진 unit vector들을 예측한다.
이 방향들은 RANSAC에 기반하여 keypoint location에 대해 vote한다.
이 voting scheme은 이미지에서 우리가 object의 local part를 볼 때 우리는 이 object의 다른 부분들에 대한 상대적 방향들을 추론할 수 있는 rigid object의 특성으로부터 동기를 얻는다.

저자들의 방법은 keypoint localization을 위해 vector field representation을 필수적으로 생성한다. 이 representation을 학습하는 것은 네트워크가 object들의 local feature들, object part들 간의 공간적(spatial) 관계들에 집중하게 강요한다.
결과적으로, 이미지 속 물체들의 안보이는 부분들의 위치는 보이는 부분들로부터 추론될 수 있다. 게다가, 이 vector field representation은 object keypoint가 input image 밖에 있더라도 표현할 수 있다. (아마 물체의 절반만 이미지에 나온다던지 그런건가?)

제안하는 방법의 또 다른 이점은 dense output들은 부정확한 keypoint 예측을 해결하기 위한 PnP solver를 위한 풍부한 정보를 제공하는 것이다. 특히 RANSAC에 기반한 voting은 잘못된(outlier) prediction들을 가지치고(prune) 각각의 keypoint에 대한 spatial probability distribution을 제공한다. keypoint location의 불확실성은 PnP solver가 final pose를 예측하기 위한 일관된 correspondence들을 식별하기 위한 좀더 많은 freedom을 제공한다.
Experiment에서 보겠지만, uncertainty 기반 PnP 알고리즘은 pose estimation의 정확도를 향상시키는 것을 보여준다.

<br/><br/>

# 3. _Proposed approach_

이 논문의 기술은 object coordinate system에서 camera coordinate system까지의 rigid transformation(R;t)로 표현되는 6D pose를 예측한다.
[32,33,39]에서 영감을 얻어 2가지 stage로 진행된다.

1. CNN을 통해 2D object keypoint를 추출한다.
2. PnP 알고리즘을 사용해 6D pose parameter들을 계산한다.

저자들의 새로운 아이디어는 2D Object keapoints를 위한 새로운 표현법과 수정된 PnP 알고리즘이다.

<br/>

## 3.1 Voting-based keypoint localization

|![그림1](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\keypointlocalization.png)|
|:--:|
|_figure 2_|

figure 2.는 keypoint localization의 전체적인 개요를 나타낸다.
RGB 이미지가 입력으로 들어오면 pvnet은 pixel단위로(pixel-wise) object label들과 unit vector들을 예측한다.
unit vector들은 모든 픽셀에서 모든 keypoint까지 방향을 나타낸 것이다.
해당 object에 속하는(belonging to) 모든 픽셀에서 특정 object keypoint로 향하는 방향(direction)이 주어지면, 저자들은 해당 object의 keypoint에 대한 2D locationsㅇ의 가설을 세우고, RANSAC 기반 voting 을 통해 confidence score를 정의한다.
이런 가설들을 기반으로, 우리는 각각의 keypoint에 대한 spatial probability distribution의 mean과 covariance를 예측한다.

_아래에서 언급하는 방법이란, pixel-wise로 direction을 예측하여 ransac을 통해 outlier를 제거하는 voting 방법_

[33,39]처럼 이미지 patch에서 keypoint location들을 직접 regressing 하는 것과 달리, 
pixel단위로 direction을 예측하는 task는 네트워크가 object들의 local feature들에 좀더 집중하게 하고, 어지러진(clutter) 배경의 영향을 줄이는 효과를 가진다.
이 방법의 또다른 이점은 물체끼리 겹쳐져 있거나(occlude) 이미지 밖에 물체의 일부가 있더라도, keypoint들을 나타낼 수 있다는 것이다.
만일 keypoint가 보이지 않을지라도, 이 방법은 정확하게 이미지 속에서 물체의 보이는 부분들에서 예측된 direction들을 통해 이 keypoint의 위치를 구할 수 있는듯.

좀더 세세하게, PVNet은 2가지 task들을 수행한다.
1. semantic segmentation : pixel p에 대해, PVNet은 semantic label, 즉 object label을 출력한다.
2. vector field predcition : 그리고 unit vector vk(p) 를 출력하는데, 이는 pixel p에서 object의 2D keypoint xk 까지의 direction을 나타낸다.

$$
v_k(p) = \frac{x_k-p}{|| x_k -p||_2}
$$

※ 주의 : vk(p)는 학습할 때 정답 keypoint xk를 주고 각 픽셀들에 대해 만든 정답 unit vector이다.

{그래서 학습된 네트워크 PVNet을 통해 이미지를 넣으면, 결과로 semantic segmentation과 vector field prediction이 나오는데, figure 2.는 거의 뭐 정답 vk를 나타내는 것이고, 실제로는 저렇게 한 점에 대해 모이지 않고 지저분하게 이미지가 나온다.
즉, 이제 아래에 쓰일 내용은 정답 keypoint xk를 추정하기 위해 하는 과정인 것임.
지저분한 이미지에서 각 픽셀들의 교차점들이 있으면 찍고 없으면 안찍는 형식으로 점을 찍는 것 같음. 그러면 figure 2에서 (e)를 보면 keypoint 주변에 점들이 찍힌 걸 볼 수 있고 외곽에 갈수록 없는 것을 볼 수 있는데, 외곽에 있는 unit vector들은 짧아서 서로 절대로 교차점이 없을 것이고, keypoint 가까이에 있는 것들은 짧지만 keypoint를 향하려는 경향이 있어서 교차점이 검은색으로 찍히게 된다. 이 검은색 점들이 hypotheses이다. 이제 아래 글을 다시 읽어보자.}

이렇게 PVNet의 결과를 통해 semantic labels과 unit vector들이 주어지면, 우린 이제 RANSAC에 기반한 voting scheme에서 keypoint hypotheses를 생성한다.
먼저, semantic label들을 사용하여 target object에 해당하는 pixel들을 찾는다.
그리고 랜덤하게 2개의 pixel들을 선택한다.
pixel 2개를 선택하고 그것들을 이용하여 교차점 (intersection)을 얻는데, 이를 hypothesis $h_{k,i}$로 지정한다.
이 단계는 hypotheses 세트 {$h_k$,i | i = 1,2, ... , N}을 만들기 위해 N번 반복한다. 이 hypotheses 세트는 가능한(possible) keypoint location들을 나타낸다.
마지막으로 object의 모든 픽셀들은 이 hypotheses에 대해 vote를 해서 voting score를 얻는다. $h_{k,i}$의 voting score $w_{k,i}$는 다음과 같이 정의된다.

![식1](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식1.png)

직관적으로, voting score $w_{k,i}$가 높은 것은 "hypothesis $h_{k,i}$가 예측된 방향 $v_k(p)$와 일치할수록 더 confident하다" 라는 것을 볼 수 있다.
{아마도 $w_{k,i}$는 두 unit vector의 dot product를 통해 같은 방향을 가르키는가? 를 보는 듯 하다. 
왜냐, theta가 0.99로 잡았다는건 거의 1에 가까워야 indicator function에서 1을 반환할 수 있는 건데, theta보다 큰 값이란건 두 유닛벡터가 거의 일치한다고 보는 것임.}

resulting hypotheses는 이미지에서 keypoint의 spatial probability distribution을 특징짓는다.
figure 2(e)는 그 예를 보여준다.

마지막으로,

![식2](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식2.png)

이것들은 나중에 Section 3.2에서 uncertainty-driven PnP 알고리즘에 쓰인다.

### Keypoint selection

|![그림2](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\figure3.png)|
|:--:|
|_figure 3_|

keypoint들은 3D object model에 기반해서 정의될 때 필요하다.
[33,39,30]과 같은 많은 방법들은 object의 3D bounding box의 8개의 코너점을 keypoint로써 사용한다. figure.3(a)가 그 예시이다.
이 코너점들은 이미지에서 object pixel들로부터 멀리 떨어져 있는게 딱봐도 보인다.
이를 사용하면 길이가 길수록 localization error가 커진다. 왜냐, 우리가 찾은 keypoint hypotheses는 object pixel에서 시작한 vector들을 사용해서 만들어지기 때문이다.
figure 3.(b)는 pvnet으로 만들어진 코너점 hypotheses이고, (c)는 object surface에서 선택된 keypoint를 나타낸다. (c)에서 본 것처럼 object surface에서의 keypoint들은 보통 localization에서 굉장히 작은 variance를 가진다.

그러므로 우리는 (b)를 사용하지 않고 (c)를 사용할 것이다.
한편, PnP 알고리즘을 좀 더 stable하게 하려면 이 keypoint들은 object에 분산되어 있어야 한다.
위 2개의 요구사항(requirements)를 생각해서, 우리는 farthest point sampling(FPS) 알고리즘을 사용하여 K 개의 keypoints를 선택한다.
먼저, 우리는 object의 중심을 추가해서 keypoint set을 초기화한다. 그리고 반복해서 object surface에서 point를 찾는다. 이 point는 현재 keypoint set에서 가장 멀리 있는 것이다. 그리고 set의 size가 K개가 될 때까지 이 point를 set에 추가한다.
section 5.3은 이 전략이 바운딩 박스의 코너점들을 사용하는 것보다 더 좋은 결과를 낸다는 것을 보여준다. 우리는 또한 키포인트들의 수를 다르게 사용하여 결과를 확인해 보았고, 우리는 K = 8 일때가 가장 trade off를 지키며 좋은 결과를 냄을 확인했다.

<br/>

## 3.2 Uncertainty-driven PnP

각 object에 대해 2D 키포인트 location들이 주어지면, off-the-shelf PnP SOLVER 를 사용하여 PnP 문제를 풀면서 6D pose를 계산할 수 있다. 예를 들면 EPnP[24]는 [39,33]에 사용됬다.
그러나 많은 방법들이 다른 keypoints이 다른 confidence들과 uncertainty patterns을 가지고 있을지도 모른다는 사실을 무시한다. 이는 PnP 문제를 풀 때 고려되어야 한다.
section 3.1에서 소개했듯이, 우리의 voting 기반 방법은 각 keypoint들에 대해 spatial probability distribution을 예측한다.
예측된 mean $\mu_k$ 와 covariance $\Sigma_k$ 가 주어지면, ( k = 1, ... ,K ) 우린 (R,t)를 계산한다. Mahalanobis distance를 최소화 하는 R,t를 말이다.

![식3](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식3.png)

R,t는 4개의 keypoints에 기반한 EPnP[24]로 초기 값을 얻는다. covariance matrices는 가장 작은 traces를 가진다.
그리고 우리가 위 식을 풀땐, LM(Levenberg-Marquardt) 알고리즘을 쓴다.
왜냐하면 Mahalanobis distance를 최소화하는 R,t를 구하는 위 식은 nonlinear 하기 때문이다.
[11]에서 approximated sampson error를 최소화 함으로써 feature uncertainties를 생각하지만, 우리는 직접 reprojection error를 최소화 했다.

<br/><br/>

# 4. _Implemetation details_

C개의 class를 가지는 object들과 각각 object마다 K 개의 keypoint들을 가진다고 가정하고
PVNet은 입력으로 H X W X 3의 이미지를 쓴다고하자.
네트워크의 진행은 fully convolutional architecture이고, output은 $H \times W \times (K \times 2 \times C)$의 tensor로 unit vector들을 표현하는 게 나오고,
$H \times W \times (C+1)$로 class probabilities가 나온다. (왜 C+1이냐면, background도 표현해줘야 해서)
우리는 pretrained ResNet-18 [16]을 backbone network (encoder)으로 사용했다. 우린 3개의 revision을 했다.

1. 학습하다가 feature map이 H/8 X W/8의 사이즈를 가질 때, 우린 pooling layer를 제거함으로써 더이상 feature map을 downsampling 하지 않았다.
2. receptive fields를 바꾸지 않고 유지하기 위해, 다음의 convolution들은 적절한 dilated convolutions으로 대체했다 [45] -> 꼭 읽어보자
3. resnet의 fully connected layer를 제거하고 convolutional layer로 대체했고, skip connection, convolution, featuremap을 upsampling 하는걸 반복하여 feature map의 크기를 H X W까지 키웠다.
그리고 1 X 1 convolution을 마지막 feature map에 사용하여 unit vector들과 class probabilities를 얻었다. (class probabilities는 segmentation을 위한 것)

앞서 chapter 3에서 했던 것처럼, 방법들을 사용했고, 초기 포즈는 opencv에서 EPnP를 사용해서 얻었다.
최종 pose를 얻기 위해, Mahalanobis distance를 최소로 하기 위한 iterative solver ceres[1]을 사용한다.
symmetric object들에 대해선, keypoint location들을 찾기 애매한 경우가 있다.
이 애매한 것을 없애기 위해, 우리는 symmetric object를 학습하는 동안 canonical pose로 회전시켰다. [33] 을 읽어보자.

## 4.1 Training strategy

우리는 unit vector들을 학습하기 위해 smooth loss l1을 사용했다.([13]에서 제안된)

![식4](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식4.png)

{위 unit vector를 학습하기 위한 loss는 다음과 같은 의미를 지닌다.
일단, $v_k$라는 것은 vector이다. 2D coordinate에선 vector가 (x,y)로 표현된다. 이를 생각하면 굉장히 간단한 loss function이 된다.
먼저 시그마가 2개가 있는데, 왼쪽의 시그마를 통해 k를 정한다. 즉, 이미지 속에서 segmentation된 k번째 object를 선택하고, 두번째 sigma로 넘어간다.
두번째 sigma는 선택된 object의 segmentation된 모든 pixel들을 하나씩 훑으면서 더해나갈거라는 의미이다.
이제 smooth loss l1 으로 vector의 변화량을 측정할 것이다.
이때, x,y의 변화량을 따로 측정할 것인데, $|_x$는 element x에 대해, $|_y$는 element y에 대해 라는 의미이다.
그리고 (p;w)가 되있는건 내가 네트워크를 통해 결과를 얻은 놈을 ~$v_k$라고 하는데 이것이 parameter w를 통해 즉, 가중치를 통해 구해진 놈이라고 표현하는 것 같다.
그래서 학습을 통해 얻은 결과 ~$v_k$와 ground truth인 $v_k$ 의 error를 구하고 다 더해서 loss를 구한다는 의미가 된다. 결국 이 loss l(w)를 줄이게 되면 당연히 $\Delta v_k$가 줄어드는 것을 의미하게 될 것이고, 이는 올바르게 unit vector를 추정해 나가는 과정이라고 볼 수 있다.}

그리고 우리는 overfitting을 막기 위해, synthetic image들을 training set에 넣었다.
각 물체마다 10000장의 이미지를 render했다. 그리고 또 다른 10000장을 사용했는데, "cut and paste"라는 전략을 이용한 것인데 [10]에서 사용한 것이다.
데이터 augmentation을 위해 random cropping, resizing, rotation, color jittering을 사용했다. 초기 learning rate는 0.001이고 20 epoch마다 절반으로 줄여나갔다. 총 200epoch를 했다.

<br/><br/>

# 5. Experiments

실험부분은 직접 논문에서 보자.