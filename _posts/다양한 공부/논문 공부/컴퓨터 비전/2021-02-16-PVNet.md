---
title: "논문 공부: PVNet : Pixel-wise Voting Network for 6DoF Pose Estimation"
excerpt : DenseFusion 공부
categories:
  - 컴퓨터 비전
  - 논문 리뷰
toc: true

---

본 글은 [논문 PVNet](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1812.11788.pdf)을 읽고 공부하는 글입니다.

---

# PVNet 개요 및 목표
---
PVNet의 새로운 점은 2D object keypoint들과 포즈 추정을 위한 수정된 PnP 알고리즘이다. 
PVNet(Pixel-wise Voting Network)는 2D keypoint들을 찾기 위해 RANSAC 같은 방법을 사용하여 겹쳐져있는 물체들에 강인함을 갖는다. RANSAC 기반 voting은 각각의 keypoint들의 spatial 확률 분포를 주며,
uncertainty-driven PnP로 6D pose를 예측하게끔 해준다.

PVNet의 목표는 이미지로부터 1) 물체들을 찾고, 2) 물체들의 3D orientation과 translation을 찾는 것이다. 

※ 여기서 6D pose란, 물체의 좌표 시스템에서 카메라 좌표 시스템까지의 rigid transformation (R;t)를 의미한다. R은 rotation, t는 translation vector를 의미한다.

물체 pose를 예측하는 것은 2단계로 이루어진다.
1. CNN을 사용해 2d 물체 keypoint들을 찾는 것
2. PnP 알고리즘을 사용해 6D pose 파라미터들을 계산하는 것

<br/><br/>
# 첫번째 단계 : Voting-based keypoint localization
---

![그림 1](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\keypointlocalization.png)

위 그림은 RGB 이미지에서 픽셀 마다의 물체 label들(d)과 unit vector(c)를 예측한다. 여기서 unit vector(c)는 각 물체에 포함되어있는 모든 픽셀에서 물체의 keypoint로의 direction을 표현한다.
우리는 이 direction들이 주어지면, RANSAC 기반 voting을 통해 keypoint에 대한 2D location과 confidence score의 가설(hypotheses)들을 생성한다. 이 가설들을 기반으로 우린 각각의 키포인트들에 대한 
spatial 확률 분포의 평균과 공분산을 예측한다. 

## Voting-based keypoint localization의 장점 

픽셀마다 direction들을 예측하는 것은 장점들이 있다.
* 네트워크가 물체들의 local 특징에 좀더 집중하게 해준다.
* 집중하게 해주므로 어수선한 배경의 효과를 덜 받게 해준다.
* 겹쳐져 있는 키포인트들을 잘 표현한다
* 심지어 화면 안을 벗어난 물체의 일부분만으로도 keypoint가 어디에 있을지 예측 가능하다.

PVNet은 이번 단계에서 semantic segmentation과 vector-field 예측을 수행한다. 
각각의 픽셀들이 (c),(d)에서 갖는 값들을 설명하기 위해, 대표적인 픽셀 p를 예시로 들자. 이 픽셀은 k번째 물체에 속해있다고 하자.

p는 semantic label k를 가질 것이고 픽셀 p에서 k번째 물체의 2D keypoint $x_k$까지의 direction을 표현하는 unit vector $v_k(p)$를 가진다.

$$
v_k(p) = \frac{x_k-p}{||x_k-p||_2}
$$

> **NOTE** : 사실 figure 2의 (d)는 완벽히 학습되었을 때의 unit vector들의 모습을 보여준다. 실제로 학습이 되지 않았다면 unit vector들이 한 점에 대해 모이지 않고 여러 방향으로 표현될 것이다.

즉, 이제 아래에 쓰일 내용은 정답 keypoint xk를 추정하기 위해 하는 과정인 것임.
지저분한 이미지에서 각 픽셀들의 교차점들이 있으면 찍고 없으면 안찍는 형식으로 점을 찍는 것 같음. 그러면 figure 2에서 (e)를 보면 keypoint 주변에 점들이 찍힌 걸 볼 수 있고 외곽에 갈수록 없는 것을 볼 수 있는데, 외곽에 있는 unit vector들은 짧아서 서로 절대로 교차점이 없을 것이고, keypoint 가까이에 있는 것들은 짧지만 keypoint를 향하려는 경향이 있어서 교차점이 검은색으로 찍히게 된다. 이 검은색 점들이 hypotheses이다. 이제 아래 글을 다시 읽어보자.}

이렇게 PVNet의 결과를 통해 semantic labels과 unit vector들이 주어지면, 우린 이제 RANSAC에 기반한 voting scheme에서 keypoint hypotheses를 생성한다.
먼저, semantic label들을 사용하여 target object에 해당하는 pixel들을 찾는다.
그리고 랜덤하게 2개의 pixel들을 선택한다.
pixel 2개를 선택하고 그것들을 이용하여 교차점 (intersection)을 얻는데, 이를 hypothesis $h_{k,i}$로 지정한다.
이 단계는 hypotheses 세트 {$h_k$,i | i = 1,2, ... , N}을 만들기 위해 N번 반복한다. 이 hypotheses 세트는 가능한(possible) keypoint location들을 나타낸다.
마지막으로 object의 모든 픽셀들은 이 hypotheses에 대해 vote를 해서 voting score를 얻는다. $h_{k,i}$의 voting score $w_{k,i}$는 다음과 같이 정의된다.

![식1](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식1.png)

직관적으로, voting score $w_{k,i}$가 높은 것은 "hypothesis $h_{k,i}$가 예측된 방향 $v_k(p)$와 일치할수록 더 confident하다" 라는 것을 볼 수 있다.
{아마도 $w_{k,i}$는 두 unit vector의 dot product를 통해 같은 방향을 가르키는가? 를 보는 듯 하다. 
왜냐, theta가 0.99로 잡았다는건 거의 1에 가까워야 indicator function에서 1을 반환할 수 있는 건데, theta보다 큰 값이란건 두 유닛벡터가 거의 일치한다고 보는 것임.}

resulting hypotheses는 이미지에서 keypoint의 spatial probability distribution을 특징짓는다.
figure 2(e)는 그 예를 보여준다.

마지막으로,

![식2](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식2.png)

이것들은 나중에 Section 3.2에서 uncertainty-driven PnP 알고리즘에 쓰인다.

### Keypoint selection

|![그림2](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\figure3.png)|
|:--:|
|_figure 3_|

keypoint들은 3D object model에 기반해서 정의될 때 필요하다.
[33,39,30]과 같은 많은 방법들은 object의 3D bounding box의 8개의 코너점을 keypoint로써 사용한다. figure.3(a)가 그 예시이다.
이 코너점들은 이미지에서 object pixel들로부터 멀리 떨어져 있는게 딱봐도 보인다.
이를 사용하면 길이가 길수록 localization error가 커진다. 왜냐, 우리가 찾은 keypoint hypotheses는 object pixel에서 시작한 vector들을 사용해서 만들어지기 때문이다.
figure 3.(b)는 pvnet으로 만들어진 코너점 hypotheses이고, (c)는 object surface에서 선택된 keypoint를 나타낸다. (c)에서 본 것처럼 object surface에서의 keypoint들은 보통 localization에서 굉장히 작은 variance를 가진다.

그러므로 우리는 (b)를 사용하지 않고 (c)를 사용할 것이다.
한편, PnP 알고리즘을 좀 더 stable하게 하려면 이 keypoint들은 object에 분산되어 있어야 한다.
위 2개의 요구사항(requirements)를 생각해서, 우리는 farthest point sampling(FPS) 알고리즘을 사용하여 K 개의 keypoints를 선택한다.
먼저, 우리는 object의 중심을 추가해서 keypoint set을 초기화한다. 그리고 반복해서 object surface에서 point를 찾는다. 이 point는 현재 keypoint set에서 가장 멀리 있는 것이다. 그리고 set의 size가 K개가 될 때까지 이 point를 set에 추가한다.
section 5.3은 이 전략이 바운딩 박스의 코너점들을 사용하는 것보다 더 좋은 결과를 낸다는 것을 보여준다. 우리는 또한 키포인트들의 수를 다르게 사용하여 결과를 확인해 보았고, 우리는 K = 8 일때가 가장 trade off를 지키며 좋은 결과를 냄을 확인했다.

<br/>

## 3.2 Uncertainty-driven PnP

각 object에 대해 2D 키포인트 location들이 주어지면, off-the-shelf PnP SOLVER 를 사용하여 PnP 문제를 풀면서 6D pose를 계산할 수 있다. 예를 들면 EPnP[24]는 [39,33]에 사용됬다.
그러나 많은 방법들이 다른 keypoints이 다른 confidence들과 uncertainty patterns을 가지고 있을지도 모른다는 사실을 무시한다. 이는 PnP 문제를 풀 때 고려되어야 한다.
section 3.1에서 소개했듯이, 우리의 voting 기반 방법은 각 keypoint들에 대해 spatial probability distribution을 예측한다.
예측된 mean $\mu_k$ 와 covariance $\Sigma_k$ 가 주어지면, ( k = 1, ... ,K ) 우린 (R,t)를 계산한다. Mahalanobis distance를 최소화 하는 R,t를 말이다.

![식3](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식3.png)

R,t는 4개의 keypoints에 기반한 EPnP[24]로 초기 값을 얻는다. covariance matrices는 가장 작은 traces를 가진다.
그리고 우리가 위 식을 풀땐, LM(Levenberg-Marquardt) 알고리즘을 쓴다.
왜냐하면 Mahalanobis distance를 최소화하는 R,t를 구하는 위 식은 nonlinear 하기 때문이다.
[11]에서 approximated sampson error를 최소화 함으로써 feature uncertainties를 생각하지만, 우리는 직접 reprojection error를 최소화 했다.

<br/><br/>

# 4. _Implemetation details_

C개의 class를 가지는 object들과 각각 object마다 K 개의 keypoint들을 가진다고 가정하고
PVNet은 입력으로 H X W X 3의 이미지를 쓴다고하자.
네트워크의 진행은 fully convolutional architecture이고, output은 $H \times W \times (K \times 2 \times C)$의 tensor로 unit vector들을 표현하는 게 나오고,
$H \times W \times (C+1)$로 class probabilities가 나온다. (왜 C+1이냐면, background도 표현해줘야 해서)
우리는 pretrained ResNet-18 [16]을 backbone network (encoder)으로 사용했다. 우린 3개의 revision을 했다.

1. 학습하다가 feature map이 H/8 X W/8의 사이즈를 가질 때, 우린 pooling layer를 제거함으로써 더이상 feature map을 downsampling 하지 않았다.
2. receptive fields를 바꾸지 않고 유지하기 위해, 다음의 convolution들은 적절한 dilated convolutions으로 대체했다 [45] -> 꼭 읽어보자
3. resnet의 fully connected layer를 제거하고 convolutional layer로 대체했고, skip connection, convolution, featuremap을 upsampling 하는걸 반복하여 feature map의 크기를 H X W까지 키웠다.
그리고 1 X 1 convolution을 마지막 feature map에 사용하여 unit vector들과 class probabilities를 얻었다. (class probabilities는 segmentation을 위한 것)

앞서 chapter 3에서 했던 것처럼, 방법들을 사용했고, 초기 포즈는 opencv에서 EPnP를 사용해서 얻었다.
최종 pose를 얻기 위해, Mahalanobis distance를 최소로 하기 위한 iterative solver ceres[1]을 사용한다.
symmetric object들에 대해선, keypoint location들을 찾기 애매한 경우가 있다.
이 애매한 것을 없애기 위해, 우리는 symmetric object를 학습하는 동안 canonical pose로 회전시켰다. [33] 을 읽어보자.

## 4.1 Training strategy

우리는 unit vector들을 학습하기 위해 smooth loss l1을 사용했다.([13]에서 제안된)

![식4](\assets\images\다양한 공부\논문\컴퓨터 비전\PVNet\식4.png)

{위 unit vector를 학습하기 위한 loss는 다음과 같은 의미를 지닌다.
일단, $v_k$라는 것은 vector이다. 2D coordinate에선 vector가 (x,y)로 표현된다. 이를 생각하면 굉장히 간단한 loss function이 된다.
먼저 시그마가 2개가 있는데, 왼쪽의 시그마를 통해 k를 정한다. 즉, 이미지 속에서 segmentation된 k번째 object를 선택하고, 두번째 sigma로 넘어간다.
두번째 sigma는 선택된 object의 segmentation된 모든 pixel들을 하나씩 훑으면서 더해나갈거라는 의미이다.
이제 smooth loss l1 으로 vector의 변화량을 측정할 것이다.
이때, x,y의 변화량을 따로 측정할 것인데, $|_x$는 element x에 대해, $|_y$는 element y에 대해 라는 의미이다.
그리고 (p;w)가 되있는건 내가 네트워크를 통해 결과를 얻은 놈을 ~$v_k$라고 하는데 이것이 parameter w를 통해 즉, 가중치를 통해 구해진 놈이라고 표현하는 것 같다.
그래서 학습을 통해 얻은 결과 ~$v_k$와 ground truth인 $v_k$ 의 error를 구하고 다 더해서 loss를 구한다는 의미가 된다. 결국 이 loss l(w)를 줄이게 되면 당연히 $\Delta v_k$가 줄어드는 것을 의미하게 될 것이고, 이는 올바르게 unit vector를 추정해 나가는 과정이라고 볼 수 있다.}

그리고 우리는 overfitting을 막기 위해, synthetic image들을 training set에 넣었다.
각 물체마다 10000장의 이미지를 render했다. 그리고 또 다른 10000장을 사용했는데, "cut and paste"라는 전략을 이용한 것인데 [10]에서 사용한 것이다.
데이터 augmentation을 위해 random cropping, resizing, rotation, color jittering을 사용했다. 초기 learning rate는 0.001이고 20 epoch마다 절반으로 줄여나갔다. 총 200epoch를 했다.

<br/><br/>

# 5. Experiments

실험부분은 직접 논문에서 보자.




