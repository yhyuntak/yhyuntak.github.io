---
title: "논문 공부: R-CNN 계열"
excerpt : R-CNN 계열에 대해 알아보자.
categories:
  - 컴퓨터 비전
  - 논문 리뷰
  - object detection
toc: true

---

기초 R-CNN은 이미지를 추론하는데 매우 느린 속도를 보였다. 이를 개선하기 위해 나온 Fast R-CNN부터 핵심만 정리하여 Mask R-CNN에 대해 간단히 설명을 적어보자.
다시 정리하는데 [약초님의 블로그](https://herbwood.tistory.com/1)를 많이 참고했다. 

# 1. Fast R-CNN 
---

## 대략적인 흐름도 
1. 이미지에서 backbone 네트워크를 적용해 feature map을 얻고, selective search도 적용해 2000개의 region proposals을 얻는다. 
2. region proposals은 이미지로부터 뽑았기 때문에, 크기가 크다. 그러나 feature map은 sub-sampling 과정을 거치면서 작아졌으므로 region proposals의 크기와 중심좌표를 sub-sampling ratio에 맞게 변형시켜준다.
3. feature map과 sub-sampling된 region proposals를 사용하여 RoI를 얻게 된다.
4. fully connected layer에 전달하기 위해 고정된 feature map을 얻어야하므로, RoI를 sub-window의 크기(전결합층에 넣기 위해 고정된 크기를 얘기함)에 맞게 grid로 나눠준다.
5. 각 채널의 grid마다 max pooling을 진행해 일정한 feature map을 만들어준다. 
6. 이 feature map의 채널이 커서 전결합층에 넘어갈 때 가중치 행렬 W가 너무 커서 연산시 속도를 지연시킨다. <br/>
예를 들어, 입력 feature map이 $7 \times 7 \times 512 $이고 4096의 feature vector를 뽑는다고 하면 가중치 행렬 W가 $25088\times 4096$의 크기를 갖는다.
이 문제를 해결하기 위해 W를 Truncated SVD를 시켜서 25088 x t 의 fc-layer와 t x 4096의 fc-layer로 쪼개서 2개의 fc-layer를 생산한다. <br/>
이렇게 되면 연산량이 기존에 25088 x 4096 이었던게 t x (25088+4096) = 25088 x t + t x 4096 로 연산량이 확 줄게 된다. 단, 정확도가 아주 약간 떨어지게 됨. 
7. 이렇게 Truncated SVD를 지난 feature vector들은 Classfier와 Bounding Box regressor layer들을 만나 (K+1) class feature vector, (K+1)x4 bbox feature vector로 나오게 된다.

## RoI Pooling
RoI Pooling은 backbone 네트워크를 거쳐 나온 feature map과 region proposals 단계에서 추천된 정보를 합쳐서 나온 RoI를 지정한 크기의 grid로 나눈 후 max pooling을 수행한다.
각 channel별로 독립적으로 수행해서 입력과 출력이 같은 채널을 갖게한다. 예를 들면 입력 RoI가 $50\times 50\times 512$ 였으면 출력 RoI는 $7 \times 7 \times 512$ 로 나온다.

## Multi-task loss
$$
Loss = L_{cls}(p,u) + \lambda[u>=1]L_{loc}(t^u,v)
$$

* $L_{cls}$는 cross entropy를 뜻한다.
* $L_{loc}$은 Smooth L1 loss를 뜻한다.
* p는 class feature vector, u는 ground truth class score, $t^u$는 bbox feature vector, $v$는 ground truth bbox location 이다.

## Hierarchical Sampling
학습시 feature sharing을 위해 서로 다른 이미지 N개의 이미지에서 총 R개의 RoI를 샘플링 한다고 하면, 각각의 이미지에서 R//N개 씩 샘플링 해서 미니배치 학습을 한다.
이 때, 각각의 이미지에서 나오는 샘플링들은 25% 정도를 ground truth와 IoU가 0.5 이상인 것으로, 75%를 IoU가 0.1~0.5인 negative한 것들로 추출한다. 
이를 통해 서로 다른 이미지에서 학습하더라도 feature sharing을 하는 경험을 하게 된다. 

end-to-end 네트워크들이 미니배치 방법을 써서 feature들이 공유가 되는 것을 모사한 것 같다.


## 장점
1. Fast R-CNN은 region proposals에 RoI pooling을 적용시켜 항상 고정된 크기의 feature vector를 Fully Connected layer에 전달할 수 있게 했다. 
2. Multi-task loss를 사용해 모델을 한번에 학습시킴으로써 학습 및 detection의 시간이 크게 감소했다.

<BR/><BR/>

# 2. Faster R-CNN 
---

## 대략적인 흐름도
1. 이미지($N\times M\times 3$) 에서 backbone 네트워크를 적용해 sub-sampling된 feature map($a\times b \time 512$)을 얻는다. 
2. 이미지에서 서로 다른 비유을 가진 anchor를 생성한다. 이 때, anchor의 개수는 ($a\ast b \ast 9$)이다.  
3. 1.의 feature map에 RPN 네트워크를 적용하여 anchor에 객체가 있는지 여부 판단 feature map ($a\times b\times 2 \times 9$)와 anchor location vector map  ($a\times b\times 4 \times 9$)를 추출한다.
4. 2,3의 결과들로 영역을 추천할 것이다. 먼저 anchor가 이미지 경계를 넘어가지 않는 것들로 추리고, class feature map의 score로 NMS를 적용해 상위 N개의 anchor만 남긴다.
5. N개의 anchor와 ground truth anchor들의 IoU에 따라 positive(>=0.5),negative(0.1<= <0.5)로 라벨링을 한다. feature sharing을 위한 sampling 때문에.
6. 1,5의 결과들로 RoI를 생성해 RoI Pooling을 거친다.
7. 나머지는 Fast R-CNN의 6,7처럼 진행한다.
8. RPN 네트워크 학습을 위해서 2.의 결과들과 ground truth anchor들을 사용해 경계를 넘지 않고 IoU가 0.7이상을 positive, IoU가 0.3 이하는 negative로 라벨링한다.
9. 학습은 RPN과 Fast R-CNN을 번갈아가며 학습한다. loss를 같이 쓰는데, RPN은 cls에서 binary CE를 쓰고, Fast R-CNN은 CE를 쓴다.

## Multi-task loss(RPN용인듯)

$$
Loss = \frac{1}{N_{cls}}\sum_i L_{cls}(p_i,p_i^*) + \lambda\frac{1}{N_{reg}}\sum_i p_i^* L_{reg}(t_i,t_i^*)
$$

* i : mini-batch 내의 anchor의 index    
* $p_i$ : anchor i에 객체가 포함되어 있을 예측 확률   
* $p_i^*$ : anchor가 양성일 경우 1, 음성일 경우 0을 나타내는 index parameter   
* $t_i$ : 예측 bounding box의 파라미터화된 좌표(coefficient)  
* $t_i^*$ : ground truth box의 파라미터화된 좌표  
* $L_{cls}$ : Loss loss  
* $L_{reg}$ : Smooth L1 loss
* $N_{cls}$ : mini-batch의 크기(논문에서는 256으로 지정)
* $N_{reg}$ :  anchor 위치의 수
* $\lambda$ : balancing parameter(default=10)

## Detection
최종적으로 얻은 predicted box에 Non maximum suppression을 적용하여 최적의 bounding box만을 결과로 출력한다.

## 장점
1. Fast R-CNN이 0.5fps(frame pre second)인 반면 Faster R-CNN 모델은 17fps를 보이며, 이미지 처리 속도 면에서 발전한 결과를 보였다.
2. Faster R-CNN은 PASCAL VOC 2012 데이터셋에서 mAP 값이 75.9를 보이면서 Fast R-CNN 모델보다 더 높은 detection 성능을 보였다. 
3. feature extraction에 사용하는 convolutional layer의 feature를 공유하면서 end-to-end로 학습시키는 것이 가능해졌다.
 
## 단점
실시간 detection에는 한계가 있다.

<br/><br/>


# 3. Mask R-CNN 
---

## 대략적인 흐름도
1. 이미지($N\times M\times 3$) 에서 ResNet-FPN backbone 네트워크를 적용해 P2,P3,P4,P5를 얻는다. 
2. 이미지에서 서로 다른 비유을 가진 anchor를 생성한다. 이 때, anchor의 개수는 ($a\ast b \ast 9$)이다.  
3. 1.의 feature map에 RPN 네트워크를 적용하여 anchor에 객체가 있는지 여부 판단 feature map ($a\times b\times 2 \times 9$)와 anchor location vector map  ($a\times b\times 4 \times 9$)를 추출한다.
4. 2,3의 결과들로 영역을 추천할 것이다. 먼저 anchor가 이미지 경계를 넘어가지 않는 것들로 추리고, class feature map의 score로 NMS를 적용해 상위 N개의 anchor만 남긴다.
5. N개의 anchor와 ground truth anchor들의 IoU에 따라 positive(>=0.5),negative(0.1<= <0.5)로 라벨링을 한다. feature sharing을 위한 sampling 때문에.
6. 1,5의 결과들로 RoI를 생성해 RoI Pooling을 거친다.
7. 나머지는 Fast R-CNN의 6,7처럼 진행한다.
8. RPN 네트워크 학습을 위해서 2.의 결과들과 ground truth anchor들을 사용해 경계를 넘지 않고 IoU가 0.7이상을 positive, IoU가 0.3 이하는 negative로 라벨링한다.
9. 학습은 RPN과 Fast R-CNN을 번갈아가며 학습한다. loss를 같이 쓰는데, RPN은 cls에서 binary CE를 쓰고, Fast R-CNN은 CE를 쓴다.

## Multi-task loss(RPN용인듯)

$$
Loss = \frac{1}{N_{cls}}\sum_i L_{cls}(p_i,p_i^*) + \lambda\frac{1}{N_{reg}}\sum_i p_i^* L_{reg}(t_i,t_i^*)
$$

* i : mini-batch 내의 anchor의 index    
* $p_i$ : anchor i에 객체가 포함되어 있을 예측 확률   
* $p_i^*$ : anchor가 양성일 경우 1, 음성일 경우 0을 나타내는 index parameter   
* $t_i$ : 예측 bounding box의 파라미터화된 좌표(coefficient)  
* $t_i^*$ : ground truth box의 파라미터화된 좌표  
* $L_{cls}$ : Loss loss  
* $L_{reg}$ : Smooth L1 loss
* $N_{cls}$ : mini-batch의 크기(논문에서는 256으로 지정)
* $N_{reg}$ :  anchor 위치의 수
* $\lambda$ : balancing parameter(default=10)

## Detection
최종적으로 얻은 predicted box에 Non maximum suppression을 적용하여 최적의 bounding box만을 결과로 출력한다.

## 장점
1. Fast R-CNN이 0.5fps(frame pre second)인 반면 Faster R-CNN 모델은 17fps를 보이며, 이미지 처리 속도 면에서 발전한 결과를 보였다.
2. Faster R-CNN은 PASCAL VOC 2012 데이터셋에서 mAP 값이 75.9를 보이면서 Fast R-CNN 모델보다 더 높은 detection 성능을 보였다. 
3. feature extraction에 사용하는 convolutional layer의 feature를 공유하면서 end-to-end로 학습시키는 것이 가능해졌다.
 
## 단점
실시간 detection에는 한계가 있다.

<br/><br/>

