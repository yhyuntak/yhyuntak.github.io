---
title: "배치 학습"
excerpt : 배치 학습에 대한 설명
categories:
  - 딥러닝
toc: true

---

# 배치 사이즈
배치 사이즈는 가중치와 편향을 수정하는 간격을 의미하는데, 이것은 학습 효율에 큰 영향을 미친다.
주로 한 epoch에 데이터를 얼만큼 묶어서 사용하는지를 보면 되는데, 데이터의 수 N을 어떻게 가져가는지에 따라 종류는 크게 3가지가 있다.
1. 전체 데이터를 한번에 학습하는 **배치 학습** $(N = the\;number\;of\;total dataset)$
2. 데이터를 한번에 하나씩만 학습하는 **온라인 학습** $(N = 1)$
3. 1과 2의 중간인 **미니 배치 학습** $(1 < N < the\;number\;of\;total dataset)$

## 배치 학습
전체 데이터셋을 한 epoch에 모든 데이터를 전부 사용하는 학습이다. 
장점들은 다음과 같다.
* 일반적으로 안정된 학습이 진행된다.
* 한 epoch에 전체 데이터를 한번에 학습하면, weight의 gradient를 선형대수를 이용해 한번에 계산할 수 있으므로 학습 속도 자체는 빠르다.

$$ E = \frac{1}{N}\Sigma_{i=1}^{N}E_i $$

$$ \frac{\partial E}{\partial w} = \Sigma_{i=1}^{N}\frac{\partial E_i}{\partial w} $$

단점은 다음과 같다.
* local minimum에 빠지기 쉽다. 

위 단점에 대한 개인적인 생각은 이렇다. 각 layer들의 Weight들을 업데이트하기 위해선 back-propagation을 진행하기위해 
gradient를 계산해야된다. 이 때, gradient는 한 epoch의 error에 대해 weight를 미분하게되고 이를 이용해서
전체 error를 줄여주는 방향으로 weight들이 업데이트가 된다. 그러나, error라는 것은 배치 학습에선 전체 데이터의 error 평균이므로
모든 error를 **한번에** 최소화하는 방향으로 가면 (음.. 네트워크란게 dimension이 너무 커서 약간 알 수 없는 blackbox의 세계이기 때문에 
말로 표현하기가 쉽진 않지만..) error를 최소화하는 고차원의 function이 0으로 깊게 들어가야하는데.. 
데이터들간의 엮인게 많아서 너무나도 쉽게 local minimum에 빠지게 되는게 아닐까 싶다. 


## 온라인 배치 학습
한 epoch에 데이터를 1개씩 사용하는 학습이다.
단점은 다음과 같다. 
* 각각의 데이터들 마다 weight가 업데이트되므로 데이터마다의 영향력이 커져 안정성이 떨어진다.

이로 인한 장점은 다음과 같다.
* (데이터간의 한번에 엮인게 풀려서인지..) local minimum에 빠지는 것을 방지할 수 있다.

## 미니 배치 학습

배치와 온라인 배치의 중간에 위치한 것이라고 생각하면 된다. 예를 들면 총 데이터의 수가 1000개면, 
한 epoch에 사용하는 데이터의 수를 무작위로 10개씩 묶어서 학습하는 것이다. 이렇게 되면 위 두 학습들의 장점들만을 취할 수 있게된다.

* 안정성이 온라인 배치 학습보다 증가한다.(배치 학습처럼)
* 상대적으로 local minumum에 빠지지 않는다.(온라인 배치 학습처럼)




