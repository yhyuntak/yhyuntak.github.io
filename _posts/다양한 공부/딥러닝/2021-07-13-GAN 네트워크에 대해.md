---
title: "GAN 네트워크에 대해"
categories:
  - 딥러닝
  - GAN
toc: True

---

**dicriminator가 실제 이미지라고 생각하면 1, 아님 0으로 출력을 내보내는 느낌. (아마 사이값을 가질 것임)
dicriminator 학습 -> generator가 학습 -> dicriminator가 학습 -> generator .. 이런순인데

dicriminator는 최대 0, 최소 -무한을 가지는데, 최대 0이 되려면 real distribution에서의 이미지는 진짜로(1) fake는 가짜(0)으로 판단해야함.
그래서 -무한에서 0으로 최대가 되게끔 만듬.

generator는 discriminator를 속이는게 목표임. 그래서 log(1-D(G(z))) 에서 D가 G(z)에 속아서 1을 출력하게끔 하고싶은 거임.
그럼 log(0)이되서 -무한이 되지. 그래서 generator는 0에서 -무한으로 만들어버리고싶은거라서 최소로 만들고 싶어하는것임.**


이 글은 GAN 네트워크를 공부하면서 생각을 기록하는 글입니다.

<br/>

# GAN 네트워크에 대해 끄적끄적
---

GAN 네트워크에 대한 설명은 여기서 자세하게 하지 않을 것 입니다. 
하지만 말하고자 하는 것을 위해 간단히 요약하자면.. 다음과 같습니다.

## GAN 네트워크

GAN 네트워크는 2개의 model을 갖고있다. 하나는 Generator, 다른 것은 Discriminator로
각각 생성자, 판별자 라고 보면 된다. 간단한 GAN 네트워크는 입력으로 임의의 랜덤 latent vector z를 사용하고 target으로 실제 image를 사용한다. 

Generator란, 임의의 latent vector z를 입력으로 받아서 Generator network를 통해 가짜 이미지 (fake)를 만들어주는 네트워크이다. 

Discriminator란, Generator가 만든 가짜 이미지(fake)와 내가 target으로 가지고 있는 진짜 이미지(real)을 구분하는 네트워크이다.

여기서 사용되는 loss는 아래와 같다.

$$ L=\frac{1}{m} \sum_{i=1}^m \{ logD(x^i)+ log(1-D(G(z^i))) \} $$

_※ m : 데이터의 수, z : latent vector, D(*) : discriminator, G(*) : generator_

sum과 m을 나누는 것은 각 데이터들의 forward-propagation을 진행하고, "평균(expectation)"을 취할 것이기 때문입니다.

위 loss를 그저 최소로 만드는 학습을 진행하면 좋겠지만.. 내가 이해한 바로는 GAN 네트워크는 조금 다른 것 같다.
GAN 네트워크는 **D가 loss를 최대로 만들게하고, G가 loss를 최소로 만들게끔 학습을 한다**는 개념을 갖는다.

Loss의 우변 첫째 항의 logD(*)에서 D(*)는 0~1의 값을 출력하게 됩니다.
여기서 Discriminator가 가짜 이미지를 만나서 확실하게 이것은 "가짜"다 라고 판단할 수 있다면 0에 가까운 값을,
"진짜"다 라고 판단할 경우엔 1에 가까운 값을 출력한다고 생각하면 됩니다. 

log는 단조증가하므로, 우변의 첫째항은 Discriminator가 "진짜"이미지 를 판별해내면 $logD(\ast)$는 0에 가까워질 것입니다.
우변의 둘째항은 가짜이미지를 판별하여 D(G(z))가 0을 출력하게 되어 $log(1-D(G(z^i)))$는 0에 가까워질 것임을 짐작할 수 있습니다. 

그러나, 무언가 수식을 곰곰히 보면서 생각해보면 이해가 안되는 부분이 있습니다.
우리의 목표는 Generator가 latent vector z 를 입력으로 받아서 "가짜" 이미지를 출력하는 것이 아닌, "진짜" 이미지를 출력하게 끔 만드는 것입니다.
근데, "진짜" 이미지에 가까운 것을 Generator가 출력하면, D(G(z))는 1을 출력할 것이고, 따라서 log(1-D(G(z)))는 $-\infty$ 로 수렴하게 됩니다.

제가 이해한 바에 따라 한가지 생각을 더하면.. 기존의 Regression, Classification 네트워크들은 loss가 0에 수렴하게끔 학습을 하지만 GAN은 그렇지 않다는 점을 알아야 합니다.
따라서 우리는 "가짜" 이미지를 "진짜" 이미지처럼 만들어서! Discriminator를 속이는 것을 두번째 목표로 잡아야 합니다. 

다시 말하자면, **Discriminator가 target 이미지 x를 정답으로 학습을 함과 동시에, Discriminator가 생성된 fake 이미지 G(z)를 보았을 때 "진짜" 인지 "가짜" 인지 모를 정도로 속여야한다는 것**입니다. 
그래서 D(G(z))가 1이 아닌 값을 출력하게끔 만들어버리는 것입니다.

위에서 "GAN 네트워크는 D가 loss를 최대로 만들게하고, G가 loss를 최소로 만들게끔 학습을 한다." 라고 소개를 했었

이제 다시 설명을 해보겠다.

먼저, Discriminator는 loss를 최대로 만든다고 했다.  

이 뜻은 다음과 같다고 생각한다.

​

Discriminator가 "진짜" 이미지를 진짜로 판단하고 "가짜" 이미지를 가짜로 판단하면, loss가 0으로 수렴하려고 할 것이다.

그러나, 우리는 두번째 목표처럼, "가짜" 이미지를 "진짜"이미지 처럼 만들어Discriminator를 속여서 Discriminator가 "진짜"를 1로, "가짜"를 0으로 판단할 수 없게끔, 마치 "나는 둘중의 하나를 고를 수 없어요. 확률은 반반일 거에요"를 표현하게끔 만들어서 D(*)를 1/2로 만들어 버리는 것이다. 그렇게되면, loss는 0으로 수렴하지 않고 0보다 큰 값으로 수렴하게 되어, loss를 "최대"로 만들려고 하는 것 같다.

​

다음으로, Generator는 loss를 최소로 만든다고 했는데, 

Generator가 계속 "진짜"같지 않은 엉뚱한 "가짜"이미지만을 만들어 낸다고 가정해보자. 

예를 들면, 사람을 만들려고하는데, 계속 강아지를 만들어내는 것처럼 말이다.

그러면 명백하게 Discriminator가 "진짜","가짜"를 쉽게 판단하여 

전체적인 loss를 -∞와 같은 값으로 출력하게 되는 것이다. 

​

그러면 loss를 "0으로 만든다. 즉, 최소로 만든다"가 아니게 되는 것이다.

그럼 Generator가 "진짜"와 같은 이미지를 만들어낸다면? 

반대로 loss는 최소로 가게끔 수렴하게 될 것이다. 

​

​

​

지금까지 GAN 네트워크에 대해서 공부하면서 이해한대로 정리를 해봤다.

그러나 이것은 순전히 내 주관적인 해석 및 판단이고, 잘못된 설명일 수도 있다. 

​

앞으로 계속 공부 및 연구를 통해 관련된 지식이 업데이트 될때마다 수정하고 

또 새로운 내용은 따로 글을 쓸 예정이다.