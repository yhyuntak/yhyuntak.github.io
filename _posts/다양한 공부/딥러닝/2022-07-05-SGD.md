---
title: "Stochastic gradient descent"
excerpt : 확률적 경사 하강(SGD)에 대한 설명
categories:
  - 딥러닝
  - Optimizer
toc: true

---

# 1. Gradient descent

경사하강법이라고 불리는 Gradient descent는 Convex loss function을 최소화 하는 weight를 찾기 위해 사용하는 optimizer이다. 
반복적으로 기울기가 깊어지는 방향으로 조금씩 움직이는 모습을 상상하면 되는데, 가장 흔한 예시로 loss function을 2차 함수를 생각하면 된다.

<br/>

## 1.1 예제
아래의 이미지는 경사 하강법의 예시를 표현한 것이다.

![2차함수](images/SGD/quadratic.png "2차함수")

2차 함수 $y=ax^2+bx+c$를 간단히 $y=(x-5)^2+5$라고 해보자. 이것의 목표는 **"y가 최소가 되는 지점은?"**이다.
그리고 위 이미지를 생각하면서 현재 파란 점의 첫 시작이 $x=3$이라고 하자. 

이제 $y=(x-5)^2+5$를 $x=3$에서 미분을 하면, 다음과 같다.

$$
\frac{dy}{dx}|_{x=3} = 2(3-5) = -4
$$

즉 $x=3$에서의 기울기는 -4가 되고, 이는 파란색 화살표 방향으로 뻗어나갈 수 있게 해주는 동력이 된다.
경사 하강법은 loss function을 최소화하는 weight를 찾는 optimizer라고 했는데 여기선 $x$가 일종의 weight라고 생각하면 된다.

경사 하강법에 의해 $x$를 업데이트 하는 식은 다음과 같다.

$$
x_{t+1} = x_{t} - \eta \frac{dy}{dx}|_{x_{t}}
$$

위 식을 이용하면 우린 $x=3$일 때를 t=1이라 하면 다음 $x_2$는 다음과 같이 구할 수 있다.

$$x_{2} = 3 + \eta * 4$$

여기서 $\eta$는 학습률(learning rate) 라고 불린다. 이 학습률은 **원하는 파라미터를 찾는데 지대한 영향**을 미친다.
왜그럴까?? 여기서 $x=3$일 때, $\eta$의 예시에 따라 결과가 어떻게 변할지를 한번 살펴보자. 

$\eta$의 예시
 1. $\eta=0.1$
 2. $\eta=0.5$
 3. $\eta=0.8$
 4. $\eta=10$

### $\eta=0.1$ 일 때
$$
x_{2}=-2(3-5)*0.1+3=3.4
$$
위 식처럼 계속해서 진행하다보면 [위 그림](#2차함수)에서 파란색 선처럼 조금씩 최적점으로 수렴하게 될 것이다.

### $\eta=0.1$ 일 때
$$
x_{2}=-2(3-5)*\textbf{0.1}+3=3.4
$$
위 식처럼 계속해서 진행하다보면 [위 그림](#2차함수)에서 <span style="color:blue">파란색 선</span>처럼 조금씩 최적점으로 수렴하게 될 것이다.

### $\eta=0.5$ 일 때
$$
x_{2}=-2(3-5)*\textbf{0.5}+3=5
$$
이번엔 우연찮게 한번에 loss function을 최소로하는 최적점을 찾아버렸다.

### $\eta=0.8$ 일 때
$$
x_{2}=-2(3-5)*\textbf{0.8}+3=6.2
$$

$$
x_{3}=-2(6.2-5)*\textbf{0.8}+6.2=4.28
$$

위 식처럼 계속해서 진행하다보면 [위 그림](#2차함수)에서 <span style="color:green">초록색 선</span>처럼 넓게 뛰면서 최적점으로 수렴하게 될 것이다.

### $\eta=10$ 일 때
$$
x_{2}=-2(3-5)*\textbf{10}+3=43
$$

위 식처럼 계속해서 진행하다보면 [위 그림](#2차함수)에서 <span style="color:red">빨간색 선</span>처럼 넓게 뛰면서 최적점으로 수렴하게 될 것이다.




아 내려가며 
배치 사이즈는 가중치와 편향을 수정하는 간격을 의미하는데, 이것은 학습 효율에 큰 영향을 미친다.
주로 한 epoch에 데이터를 얼만큼 묶어서 사용하는지를 보면 되는데, 데이터의 수 N을 어떻게 가져가는지에 따라 종류는 크게 3가지가 있다.
1. 전체 데이터를 한번에 학습하는 **배치 학습** $(N = the\;number\;of\;total dataset)$
2. 데이터를 한번에 하나씩만 학습하는 **온라인 학습** $(N = 1)$
3. 1과 2의 중간인 **미니 배치 학습** $(1 < N < the\;number\;of\;total dataset)$

<br/>

## 1.1 배치 학습
전체 데이터셋을 한 epoch에 모든 데이터를 전부 사용하는 학습이다. 
장점들은 다음과 같다.
* 일반적으로 안정된 학습이 진행된다.
* 한 epoch에 전체 데이터를 한번에 학습하면, weight의 gradient를 선형대수를 이용해 한번에 계산할 수 있으므로 학습 속도 자체는 빠르다.

$$ E = \frac{1}{N}\Sigma_{i=1}^{N}E_i $$

$$ \frac{\partial E}{\partial w} = \Sigma_{i=1}^{N}\frac{\partial E_i}{\partial w} $$

단점은 다음과 같다.
* local minimum에 빠지기 쉽다. 

위 단점에 대한 개인적인 생각은 이렇다. 각 layer들의 Weight들을 업데이트하기 위해선 back-propagation을 진행하기위해 
gradient를 계산해야된다. 이 때, gradient는 한 epoch의 error에 대해 weight를 미분하게되고 이를 이용해서
전체 error를 줄여주는 방향으로 weight들이 업데이트가 된다. 그러나, error라는 것은 배치 학습에선 전체 데이터의 error 평균이므로
모든 error를 **한번에** 최소화하는 방향으로 가면 (음.. 네트워크란게 dimension이 너무 커서 약간 알 수 없는 blackbox의 세계이기 때문에 
말로 표현하기가 쉽진 않지만..) error를 최소화하는 고차원의 function이 0으로 깊게 들어가야하는데.. 
데이터들간의 엮인게 많아서 너무나도 쉽게 local minimum에 빠지게 되는게 아닐까 싶다. 

<br/>

## 1.2 온라인 배치 학습
한 epoch에 데이터를 1개씩 사용하는 학습이다.
단점은 다음과 같다. 
* 각각의 데이터들 마다 weight가 업데이트되므로 데이터마다의 영향력이 커져 안정성이 떨어진다.

이로 인한 장점은 다음과 같다.
* (데이터간의 한번에 엮인게 풀려서인지..) local minimum에 빠지는 것을 방지할 수 있다.

<br/>

## 1.3 미니 배치 학습

배치와 온라인 배치의 중간에 위치한 것이라고 생각하면 된다. 예를 들면 총 데이터의 수가 1000개면, 
한 epoch에 사용하는 데이터의 수를 무작위로 10개씩 묶어서 학습하는 것이다. 이렇게 되면 위 두 학습들의 장점들만을 취할 수 있게된다.

* 안정성이 온라인 배치 학습보다 증가한다.(배치 학습처럼)
* 상대적으로 local minumum에 빠지지 않는다.(온라인 배치 학습처럼)




