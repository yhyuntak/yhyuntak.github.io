---
title: "나이브 베이지안 분류기"
excerpt : 나이브 베이지안 분류기(Naive Bayes Classifier)에 대한 글입니다.
categories:
  - 데이터 마이닝
  - 머신러닝
  - 분류기
toc: true
---

# 나이브 베이지안 분류기(Naive Bayes Classifier)
---

많은 분류 문제에는 다음과 같은 **불확실성들이 수반**된다.

1. feature를 측정하는 장치에 정확도 이슈가 있어서, feature와 클래스 레이블의 관계를 신뢰할 수 없다.<br/>
2. 데이터의 feature set들이 각각의 클래스 레이블들을 대표하지 않을 수 있다.<br/>
  ex) 매일 운동하고, 밥을 3끼 잘 먹으면 건강하다. 라는 분류를 할때, 흡연,술 등의 feature들은 포함되어 있지 않다. 따라서 
  운동,밥3끼의 특징들이 건강하다의 분류에 모든 경우를 대표하지 않는다.  <br/>
3. 한정된 훈련 데이터로는 실제 클래스 레이블들과의 관계를 완전히 포착하지 못할 수 있다. <br/>
4. 실제 시스템의 무작위 특성(random nature)로 인해 예측의 불확실성이 발생할 수 있다.<br/>

**불확실성이 존재하는 경우, 클래스 레이블을 예측하는 것 외에 예측과 관련된 신뢰도를 제공**해야한다!
확률 이론은 데이터의 불확실성을 정량화하고 조작하는 체계적인 방법을 제공하므로 예측의 신뢰성을 평가하기 위한 매력적인 프레임워크다.

확률 이론을 사용해 feature들과 클래스간의 관계를 나타내는 분류 모델을 **확률적 분류 모델**이라고 한다. 

가장 단순하고 가장 널리 사용되는 모델이 바로 **나이브 베이지안 분류기**이다.

본격적으로 나이브 베이지안 분류기에 대해 알기 전에, 간단한 확률 이론과 베이즈 정리에 대해 모른다면 [게시글](https://yhyuntak.github.io/%ED%8C%A8%ED%84%B4%20%EC%9D%B8%EC%8B%9D%EA%B3%BC%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D/%EA%B0%84%EB%8B%A8%ED%95%9C-%ED%99%95%EB%A5%A0-%EA%B0%9C%EB%85%90%EB%93%A4/)을 보고오자.

<br/>

## 분류를 위한 베이지 정리 사용

우리는 훈련 데이터 셋의 feature들(x)이 주어지면, 이것들을 사용해서 클래스 y가 나올 확률에 대해 궁금하다. 이것을 식으로 표현하면 다음과 같다.

$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$

여기서 $p(x|y)$에 집중해보자. 굉장히 중요한 개념이다. 이것은 **클래스 조건부(class-conditional) 확률**로 불리운다.
말 그대로 클래스 레이블 y가 주어졌을 때, x라는 feature vector가 나올 확률은 어떤가? 를 묻는 것이다.

만약 실제로 x가 y에 속하는 경우엔 $p(x\|y)$가 높을 것으로 예상된다. 왜냐하면, 베이지안 식에서 $p(y\|x) \propto p(x\|y)$이기 때문이다.

위 관점에서 클래스 조건부 확률을 사용하면, 데이터 객체 x가 생성된 과정을 포착하려고 시도한다. 이런 확률 분류 모델을 **생성 분류 모델(generative classification models)**이라고 한다!

우변의 분자의 두번째 항 $p(y)$는 **사전 확률(prior probability)**라고 한다. 분모 $p(x)$는 marginal prob로 구할 수 있는데, 사후 확률 $p(y\|x)$를 확률 값 범위 0~1로 바꿔주는 정규화 상수이다.

### 계산법

* 사전 확률 $p(y)$
단순하게 각 클래스에 속하는 데이터 객체들의 비율로 표현하면 된다. 

$$
p(y_i) = \frac{n_i}{n}
$$

* **클래스 조건부 확률 $p(x\|y)$**
예시로 먼저 보자. 예를 들어 $c_1,...,c_k$의 값을 갖는 feature가 $X_1,X_2$가 있다고 해보자. 그럼 $p(x\|y)$ 는 다음과 같이 표현된다.

$$
p(x|y) = p(X_1=c_i,\,X_2=c_j|Y=0) = \frac{n_{ij}^0}{n^0}
$$

※ $n_{ij}^0$은 클래스 0에 속하면서 X_1=c_i,\,X_2=c_j의 feature를 갖는 데이터의 수를 의미하고, $n^0$는 클래스 0의 개수를 의미한다.

식은 간단하지만, 사실 feature의 차원이 D, 각각의 값들이 K개를 가질 수 있다고 한다면, feature들의 조합이 $K^D$가 된다. 즉, 차원이 커질수록 계산이 불가능해질 것이 틀림없으며,
조합의 개수는 많은데, 훈련 데이터가 적다면 성능을 더욱 떨어트릴 것이다.

이 문제를 해결하기 위해서! 우리는 나이브 베이즈 가정을 하게 된다.

<br/>

## 나이브 베이즈 가정

나이브 베이즈 가정을 사용하면, 클래스 조건부 확률은 다음과 같이 바뀐다.

$$
p(x|y) = \Pi_{i=1}^d p(x_i|y)
$$

나이브 베이즈 가정은 각각의 feature들이 서로 조건부 독립이라고 가정하는 것을 뜻한다. 

### 조건부 독립

$X_1$과 $X_2$가 서로 독립적이면, 다음의 조건이 유지된다.

$$
p(X_1|X_2,Y)=p(X_1|Y)
$$

그러면 결합 조건부 확률 $p(X_1,X_2\|Y)$는 다음과 같이 풀이된다.

$$
\begin{align*}
p(X_1,X_2|Y) &= \frac{p(X_1,X_2,Y)}{p(Y)}  \\
&= \frac{p(X_1,X_2,Y)}{p(X_2,Y)} \times \frac{p(X_2,Y)}{p(Y)} \\
&= p(X_1|X_2,Y) \times p(X_2|Y) \\
&= p(X_1|Y) \times p(X_2|Y)
\end{align*}
$$

위 식은 앞서 언급했던 나이브 베이즈 가정의 기초가 된다.

* 나이브 베이즈 가정 

  $$
  p(x|y) = \Pi_{i=1}^d p(x_i|y)
  $$

### 나이브 베이즈 분류기 작동 방식

나이브 베이즈 가정을 이용하면, 우리는 클래스 $y$에서 feature $x_i$의 조건부 확률만 계산하면 된다.
예를 들어, $n_i^0$,$n_j^0$이 각각 클래스 0에 속하는 feature들이 $X_1=c_i,X_2=c_j$인 훈련 데이터 수를 나타낸다고 하면, 클래식 조건부 확률은 다음과 같이 쓸 수 있다.

$$
p(X_1=c_i,X_2=c_j|Y=0) = \frac{n_i^0}{n^0} \times \frac{n_j^0}{n^0}
$$

이렇게 되면, 클래스 조건부 확률을 학습하는데 필요한 매개변수의 수가 기존의 $D^K$에서 $DK$로 줄게된다. 따라서 고차원적 설정에도 매개 변수들을 학습하고 예측하는데 더 적합해진다.

나이브 베이즈 가정을 이용해 사후 확률 $p(y\|x)$를 다시 써보자.

$$
p(y|x) = \frac{p(y)\Pi^d_{i=1} p(x_i|y)}{p(x)}
$$

분모는 정규화 항이므로 우리는 분자항 $p(y)\Pi^d_{i=1} p(x_i\|y)$에 주목하면 되고, 이것을 최대화하는 클래스를 선택하는 것이 나이브 베이즈 분류기의 추론이다.


<br/>

## 특징 정리

* 나이브 베이즈 분류기는 사후 확률 추정치를 제공해 **예측의 불확실성을 정량화 할 수 있는 확률적 분류 모델**이다.
* 대상 클래스를 데이터 객체 생성을 위한 원인으로(클래스 조건부 확률) 취급하므로 생성 분류 모델이다.
* feature들이 클래스에 의해 조건부 독립인 경우, 고차원 feature 공간에서도 클래스 조건 확률을 쉽게 계산할 수 있다. (텍스트 분류같은 곳에서 자주 사용된다.)
* 결측 값을 무시할 수 있어서 결측 값에 강하다.
* 클래스를 분류하는 것에 대해 관련이 없는 feature들은 모든 클래스에 대해 거의 균일하게 분포되어 관련 없는 feature들에 강하다.
* 중복 또는 서로 상호 작용하는 feature이 존재할 경우, 독립적이지 않기 때문에 성능이 저하된다.