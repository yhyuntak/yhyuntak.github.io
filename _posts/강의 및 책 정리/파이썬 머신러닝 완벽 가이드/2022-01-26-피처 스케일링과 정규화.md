---
title: "피처 스케일링과 정규화"
categories:
  - 파이썬 머신러닝 완벽 가이드
toc: true
---

데이터들을 학습시키기 전에 우리는 반드시 데이터를 스케일링 혹은 정규화를 해야할 것입니다. 
딥러닝에서 기본적으로 이미지 데이터는 항상 normalize를 하는 편인데, 그 이유는 무엇일까요?

데이터를 이미지라고 가정했을 때, 한 픽셀당 0~255의 값을 가질 수 있습니다.(흑백사진에서) 

앞서 [데이터 전처리 (encoding편)]()에서 label encoding을 생각하면 숫자의 크기에 따라 인공지능의 학습이 한쪽으로 치우쳐질 가능성이 있다는 것을 이야기 했었습니다. 

한 픽셀이 0~255 값을 가지는데 당연히 이또한 문제가 생기기 마련이다. 따라서 우리는 값들을 같은 범위내에서 학습을 하는 것이 중요하다. 이 때 사용하는 방식이 정규화이다. 책에는 StandardScaler와 MinMaxScaler를 소개한다. 전자는 데이터들을 정규분포 N~(0,1)을 따르게 끔 데이터를 스케일링 하는 함수이고, 후자는 최대 최소를 1,0이 되게하는 (양수일 때이고 음수일땐 1,-1로) 함수이다. 경우에 따라서 normalize하는 방식이 달라지겠지만, StandardScaler의 경우, SVD,Linear Regression,Logisitic Regression과 같은 기법들이 데이터가 가우시안 분포를 따른다고 가정하고 진행되기 때문에, 성능 향상을 위해서 거의 반드시 사용되어야 하는 스케일링 함수임을 여기서 한번 언급한다.


특히, 데이터를 변환할 때 학습데이터와 테스트데이터의 스케일링은 항상 같은 범위로 되어야 함을 잊지 않아야한다. (예를 들어, 학습 데이터에서 10이 0.1로 스케일링이 되었다면, 테스트 데이터의 10의 값은 0.1로 스케일링이 되어야한다는 뜻)
