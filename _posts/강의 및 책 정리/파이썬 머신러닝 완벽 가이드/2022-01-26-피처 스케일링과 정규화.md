---
title: "피처 스케일링과 정규화"
categories:
  - 파이썬 머신러닝 완벽 가이드
toc: true
---

데이터들을 학습시키기 전에 우리는 반드시 데이터를 스케일링 혹은 정규화를 해야할 것입니다. 
딥러닝에서 기본적으로 이미지 데이터는 항상 normalize를 하는 편인데, 그 이유는 무엇일까요?

데이터를 이미지라고 가정했을 때, 한 픽셀당 0~255의 값을 가질 수 있습니다.(흑백사진에서) 

앞서 [데이터 전처리 (encoding편)](https://yhyuntak.github.io/%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%99%84%EB%B2%BD%20%EA%B0%80%EC%9D%B4%EB%93%9C/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC(encoding%ED%8E%B8)/)에서 label encoding을 생각하면 숫자의 크기에 따라 인공지능의 학습이 한쪽으로 치우쳐질 가능성이 있다는 것을 이야기 했었습니다.
한 픽셀이 0~255 값을 가지는데 당연히 이또한 문제가 생길 것입니다. 따라서 우리는 값들을 같은 범위 내에서 학습을 하는 것이 중요하다는 것을 알게 되었습니다. 
이 때 사용하는 방식이 정규화(Normalization)입니다. 책에는 StandardScaler와 MinMaxScaler를 소개합니다. 

* StandardScaler : 데이터들이 정규분포 N~(0,1)을 따를 수 있게 데이터들을 스케일링 하는 함수
* MinMaxScaler : 최대,최소 값들이 1,0이 되게하는 (양수일 때이고 음수일땐 1,-1로) 함수
 
StandardScaler의 경우에는 SVD,Linear Regression,Logisitic Regression과 같은 기법들이 *데이터가 가우시안 분포를 따른다고 가정*하고 진행되기 때문에, 
성능 향상을 위해서 거의 반드시 사용되어야 하는 스케일링 함수라고 합니다.

특히, 데이터를 변환할 때  **학습데이터와 테스트데이터의 스케일링은 항상 같은 범위로 되어야 함**을 잊지 않아야 합니다.
예를 들어, 학습 데이터에서 10이 0.1로 스케일링이 되었다면, 테스트 데이터의 10의 값은 0.1로 스케일링이 되어야한다는 뜻입니다!
