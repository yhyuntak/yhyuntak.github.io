---
title: "제 2장 : 자연어 처리의 기본에 대해"
categories:
  - 밑바닥부터 시작하는 딥러닝 시리즈
  - NLP
toc: true
excerpt : 2장을 읽고 정리한 글입니다.
---

# 자연어란?
---

자연어(natural language)란 우리가 평소에 쓰는 말을 일컫는다. 
따라서 자연어 처리(Natural Language Processing) 통칭 NLP란 **자연어를 컴퓨터가 처리해서 우리의 말을 이해할 수 있도록 하는 것**이라 보면 될 것 같다. 

사람의 말하는 것의 의미는 "단어"로 구성된다. 
예를 들면, 어머니 라는 뜻은 우리를 낳아주신 여성이라고 이해할 수 있다. 그러나 어,머,니 를 각각 따로 보면 무슨 말인지 모를 것이다. 
따라서 단어는 단어로써 의미가 이해되기 때문에, 그대로 컴퓨터에 접목시켜 단어 자체를 학습시키는게 목표가 된다. 

<br/><br/>

# 분포 가설이란?
---

책의 예제는 영어지만 간단하게 내가 이해한 바를 표현하기 위해 한글로 예시를 만들어서 써보려고 한다. 
"나는 시계가게에서 물건을 샀고 시계를 보니, 어느덧 벌써 시간이 지나갔다." 라는 글을 봐보자. 
저자는 많은 NLP연구들이 "분포 가설"이라는 것에 근간을 둔다고 한다. 
이것은 **단어의 의미는 주변 단어들을 통해 이루어진다는 것**을 의미한다. 

위 문장을 바꿔서, 만약 "000 00 000 지나갔다" 라고 글을 썼다고 해보자. 
우리는 뭐가 지나갔다는 건지 모른다. _사람이 지나간 것인가? 새가 지나갔나??_ 싶다. 
그러나 단순히 "시간이 지나갔다"라고만 표현하더라도 _아, 시간이 내가 생각한거보다 더 오래 걸렸구나.._ 라는 생각을 하게 된다.
이렇듯 단어들끼리 묶여있어야 각각의 단어가 무엇을 의미하는지 알게 되는 것이다.

<br/><br/>

# 분산 표현이란?
---

컴퓨터에서 모델을 학습할 때, 데이터를 string으로써 학습하는 것은 사실상 불가능하다. 
가장 좋은 대체 방법은 **단어들을 vector로 표현하는 것**이다. 
책에서 RGB개념을 예시로 좋은 설명을 써주었다.

> 우리가 파랑이라고 생각하는 것을 컴퓨터에게 표현하라고 하면 컴퓨터는 벙찔것이다. 
그러나 우리가 RGB 벡터로 (0,0,255)를 생각하라고 하면 컴퓨터는 즉각 반응할 것이다. 

위 예시처럼 컴퓨터에 우리의 언어를 표현하고 싶으면, **단어의 의미를 정확하게 파악할 수 있는 vector 표현을 갖는 것**이 중요하다. 
이것을 자연어 처리에선 **단어의 의미를 정확하게 파악할 수 있는 벡터 표현을 분산 표현** 이라고 한다.

<br/><br/>

# 동시발생 행렬과 cosine similarity
---

한 문장의 각 단어들의 좌우를 탐색해서 동시에 발생하는 단어들을 묶은 벡터들을 모든 단어에 대해 row방향으로 쌓으면 동시발생 행렬(co-occurrence matrix)을 얻을 수 있다.
각 row 벡터들은 단어 벡터이고, 단어간의 유사도는 단어 벡터간의 cosine similarity를 계산해서 확인할 수 있다.

<br/><br/>

# 동시발생 행렬과 cosine similarity
---

단어를 발생 횟수로만 단어간의 유사성을 확인하려한다면 관사 + 명사에 의해 관사와 명사는 유사하다고 표현될 수 있다. 문제를 해결하기 위해 
**점별 상호정보량(Pointwise Mutual Information)(PMI)**을 사용해 각각의 단어들의 출현빈도가 적더라도, 동시 발생의 수가 많은 것을 중요하다고 표현한다.

**단어와 단어사이의 비어있는 공간에 단어를 예측하는 방법**으로 

<br/><br/>

# 차원 축소
---

하지만 PMI는 element들이 주로 0으로만 표현된 sparse matrix이다. 
따라서 이 sparse matrix를 그대로 사용하는 것보단 **해당 벡터들의 핵심 feature들을 표현하는 차원이 축소된 matrix를 사용하는 것이 계산적으로 더 효율적**일 것이다. 이것을 SVD를 통해서 이뤄보고자 한다. 

SVD로 sparse matrix를 decomposition을 하고 **singular value가 가장 큰 축에 해당하는 U의 열벡터들을 "차원이 감소된 단어 벡터"로써 사용**한다. 
왜냐하면 **행렬을 SVD를 한 후, singular value가 큰 열벡터들은 해당 행렬의 특징을 가장 잘 표현하는 basis가 되기 때문**이다. 
그래서 실제로 singular value가 큰 U,V의 행,열벡터들만을 이용하여 $USV^T$를 진행하면 원래 행렬과 굉장히 유사하게 복원되는 것을 볼 수 있다. 이것을 **Truncated SVD**라고 합니다.

<br/><br/>

# 마치며
---

위 방식들을 통틀어서 "통계 기반 기법"이라고 하는데, 이 방법의 **단점은 모든 데이터를 "한번에" 처리하는 것**이다. 
발생하는 문제는 다음과 같다.
* 시간이 오래걸림
* 새로운 입력 데이터를 추가해서 학습하려면 그때마다 매번 전체 PMI를 계산하고 SVD를 해야함.